{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pig: Índice invertido\n",
    "\n",
    "Partiendo del dataset de posts utilizado anteriormente, vamos a calcular un índice invertido.\n",
    "\n",
    "Este notebook se ejecutará dentro del cluster de Hadoop creado usando Docker durante las clases del máster. En concreto, se usarán las siguientes imágenes:\n",
    "* accaminero/namenode01\n",
    "* swapnillinux/cloudera-hadoop-yarnmaster\n",
    "* swapnillinux/cloudera-hadoop-datanode\n",
    "\n",
    "Se puede construir el cluster Hadoop y lanzar el entorno Jupyter usando los siguientes scripts incluídos en la práctica:\n",
    "* docker/build.sh\n",
    "* docker/jupyter.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparación del entorno de ejecución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una carpeta local para almacenar los ficheros de trabajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/notebooks/pig-indiceinvertido/pig-indiceinvertido/pig-indiceinvertido\r\n"
     ]
    }
   ],
   "source": [
    "! rm -fr pig-indiceinvertido\n",
    "! mkdir -p pig-indiceinvertido\n",
    "import os\n",
    "os.chdir(\"pig-indiceinvertido/\")\n",
    "! pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instalación de depencias:\n",
    "* Instalamos dos2unix para limpiar el fichero y convertirlo de formato DOS a Unix\n",
    "* Instalamos pig para ejecutar los correspondientes comandos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded plugins: fastestmirror, ovl\n",
      "base                                                     | 3.6 kB     00:00     \n",
      "cloudera-cdh                                             |  951 B     00:00     \n",
      "epel/x86_64/metalink                                     |  28 kB     00:00     \n",
      "epel                                                     | 4.7 kB     00:00     \n",
      "extras                                                   | 3.4 kB     00:00     \n",
      "updates                                                  | 3.4 kB     00:00     \n",
      "(1/2): epel/x86_64/updateinfo                              | 881 kB   00:02     \n",
      "(2/2): epel/x86_64/primary_db                              | 6.2 MB   00:06     \n",
      "Determining fastest mirrors\n",
      " * base: mirror.tedra.es\n",
      " * epel: mir01.syntis.net\n",
      " * extras: mirror.tedra.es\n",
      " * updates: mirror.tedra.es\n",
      "Package dos2unix-6.0.3-7.el7.x86_64 already installed and latest version\n",
      "Package pig-0.12.0+cdh5.9.0+95-1.cdh5.9.0.p0.30.el7.noarch already installed and latest version\n",
      "Package hbase-1.2.0+cdh5.9.0+205-1.cdh5.9.0.p0.30.el7.x86_64 already installed and latest version\n",
      "Nothing to do\n"
     ]
    }
   ],
   "source": [
    "! yum install -y dos2unix pig hbase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copiamos los ficheros de datos al directorio de trabajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 38M\r\n",
      "-rw-r--r-- 1 root root 1.8K Feb 17 19:10 forum1.tsv\r\n",
      "-rwxr-xr-x 1 root root  38M Feb 17 19:10 forum_node.tsv.gz\r\n"
     ]
    }
   ],
   "source": [
    "! cp ../dataset/forum_node.tsv.gz ../dataset/forum1.tsv .\n",
    "! ls -lh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descomprimimos el primer fichero y lo limpiamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dos2unix: converting file forum_node.tsv to Unix format ...\r\n"
     ]
    }
   ],
   "source": [
    "! gzip -d forum_node.tsv.gz && dos2unix -f forum_node.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el directorio de usuario en Hadoop si no existiera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/root/pig-indiceinvertido': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -rm -r /user/$(whoami)/pig-indiceinvertido\n",
    "! hadoop fs -mkdir -p /user/$(whoami)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copiamos los ficheros a Hadoop y al directorio local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `forum_node.tsv': File exists\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -put -p forum_node.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `forum1.tsv': File exists\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -put forum1.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\r\n",
      "-rw-r--r--   3 root supergroup       1774 2018-02-05 15:34 forum1.tsv\r\n",
      "-rwxr-xr-x   3 root root        120109135 2018-02-05 15:33 forum_node.tsv\r\n",
      "drwxr-xr-x   - root supergroup          0 2018-02-05 15:42 inverted_index\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creación del fichero PIG a ejecutar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing students-inverted-index.pig\n"
     ]
    }
   ],
   "source": [
    "%%writefile students-inverted-index.pig\n",
    "\n",
    "/* 1.Carga el fichero de los posts forum_node.tsv, utilizando una extension de Piggybank para poder quitar la cabecera,\n",
    "en vez de usar directamente el PigStorage. */\n",
    "REGISTER /usr/lib/pig/piggybank.jar;\n",
    "DEFINE StringToInt InvokeForInt('java.lang.Integer.valueOf', 'String');\n",
    "\n",
    "data =\n",
    "    load 'forum_node.tsv'\n",
    "    using org.apache.pig.piggybank.storage.CSVExcelStorage('\\t', 'YES_MULTILINE', 'NOCHANGE', 'SKIP_INPUT_HEADER')\n",
    "    as (pid:chararray, title:chararray, tagnames:chararray,\n",
    "        author_id:chararray,body:chararray,\n",
    "        node_type:chararray, parent_id:chararray,\n",
    "        abs_parent_id:chararray,added_at:chararray,\n",
    "        score:chararray, state_string:chararray, last_edited_id:chararray,\n",
    "        last_activity_by_id:chararray, last_activity_at:chararray,\n",
    "        active_revision_id:chararray, extra:chararray,\n",
    "        extra_ref_id:chararray, extra_count:chararray, marked:chararray);\n",
    "\n",
    "/* 2.Limpiamos el fichero quitando los saltos de linea, expresiones html y la expresión regular que se proponia en el ejercicio. */\n",
    "cleandata = foreach data generate\n",
    "    REPLACE(pid, '[a-zA-Z]+', '') as post_id,\n",
    "    LOWER(REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(body, '\\\\\\\\n\\\\\\\\r', ''), '\\\\\\\\r', ''), '\\\\\\\\n', ''), '<*>', ''), '[^a-zA-Z0-9\\'\\\\s]+', ' ')) AS clean_body;\n",
    "\n",
    "/* 3.Filtramos los datos de post_id que no son numericos. */\n",
    "cleandata_filtered = filter cleandata by org.apache.pig.piggybank.evaluation.IsNumeric(post_id);\n",
    "\n",
    "/* 4.Creamos tuplas separando el body por espacios y convirtiendo el post_id en un numerico a través de una función custom, para evitar problemas que sufrimos con el cast de String a Integer. */\n",
    "words_data = FOREACH cleandata_filtered GENERATE StringToInt(post_id) as post_id_int:int, FLATTEN(TOKENIZE(clean_body)) as word;\n",
    "words_data_filtered = filter words_data by SIZE(word) > 0;\n",
    "\n",
    "/* 5.Agrupamos por palabra */\n",
    "word_groups = GROUP words_data_filtered BY word;\n",
    "\n",
    "/* 6.Por cada grupo de palabras, hacemos un distinct para los post_id, eliminando los duplicados, contamos el número de post en que aparece (despues de quitar los duplicados) y generamos una fila con el índice. */\n",
    "index = FOREACH word_groups {\n",
    "    pairs = DISTINCT $1.$0;\n",
    "    cnt = COUNT(pairs);\n",
    "    GENERATE $0 as word, pairs as index_bag, cnt as count;\n",
    "};\n",
    "\n",
    "/* 7.Como se pide que el indice lleve el post_id ordenador, ordenamos la bag resultante de los posts por su id. */\n",
    "sorted_index = foreach index {\n",
    "    sorted_bag = order index_bag by $0;\n",
    "    generate word, sorted_bag, count;\n",
    "}\n",
    "\n",
    "/* 8. Lo guardamos en un fichero. */\n",
    "STORE sorted_index INTO 'inverted_index';\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ejecución del fichero PIG en modo local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log4j:WARN No appenders could be found for logger (org.apache.hadoop.util.Shell).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n",
      "2018-02-17 19:11:49,861 [main] INFO  org.apache.pig.Main - Apache Pig version 0.12.0-cdh5.9.0 (rUnversioned directory) compiled Oct 21 2016, 01:17:18\n",
      "2018-02-17 19:11:49,862 [main] INFO  org.apache.pig.Main - Logging error messages to: /media/notebooks/pig-indiceinvertido/pig_1518894709834.log\n",
      "2018-02-17 19:11:49,883 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "2018-02-17 19:11:50,240 [main] INFO  org.apache.pig.impl.util.Utils - Default bootup file /root/.pigbootup not found\n",
      "2018-02-17 19:11:50,359 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-17 19:11:50,361 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "2018-02-17 19:11:50,361 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: file:///\n",
      "2018-02-17 19:11:50,449 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-17 19:11:50,526 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-17 19:11:50,591 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-17 19:11:50,653 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-17 19:11:50,731 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-17 19:11:50,816 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-17 19:11:50,871 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-17 19:11:50,920 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-17 19:11:51,021 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-17 19:11:51,877 [main] WARN  org.apache.pig.PigServer - Encountered Warning IMPLICIT_CAST_TO_LONG 1 time(s).\n",
      "2018-02-17 19:11:51,895 [main] INFO  org.apache.pig.tools.pigstats.ScriptState - Pig features used in the script: GROUP_BY,FILTER\n",
      "2018-02-17 19:11:51,974 [main] INFO  org.apache.pig.newplan.logical.optimizer.LogicalPlanOptimizer - {RULES_ENABLED=[AddForEach, ColumnMapKeyPrune, DuplicateForEachColumnRewrite, GroupByConstParallelSetter, ImplicitSplitInserter, LimitOptimizer, LoadTypeCastInserter, MergeFilter, MergeForEach, NewPartitionFilterOptimizer, PushDownForEachFlatten, PushUpFilter, SplitFilter, StreamTypeCastInserter], RULES_DISABLED=[FilterLogicExpressionSimplifier, PartitionFilterOptimizer]}\n",
      "2018-02-17 19:11:52,023 [main] INFO  org.apache.pig.newplan.logical.rules.ColumnPruneVisitor - Columns pruned for data: $1, $2, $3, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15, $16, $17, $18\n",
      "2018-02-17 19:11:52,046 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.textoutputformat.separator is deprecated. Instead, use mapreduce.output.textoutputformat.separator\n",
      "2018-02-17 19:11:52,184 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler - File concatenation threshold: 100 optimistic? false\n",
      "2018-02-17 19:11:52,235 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size before optimization: 1\n",
      "2018-02-17 19:11:52,235 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size after optimization: 1\n",
      "2018-02-17 19:11:52,297 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "2018-02-17 19:11:52,301 [main] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "2018-02-17 19:11:52,357 [main] INFO  org.apache.pig.tools.pigstats.ScriptState - Pig script settings are added to the job\n",
      "2018-02-17 19:11:52,479 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.reduce.markreset.buffer.percent is deprecated. Instead, use mapreduce.reduce.markreset.buffer.percent\n",
      "2018-02-17 19:11:52,479 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3\n",
      "2018-02-17 19:11:52,479 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress\n",
      "2018-02-17 19:11:52,483 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Reduce phase detected, estimating # of required reducers.\n",
      "2018-02-17 19:11:52,485 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Using reducer estimator: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.InputSizeReducerEstimator\n",
      "2018-02-17 19:11:52,490 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.InputSizeReducerEstimator - BytesPerReducer=1000000000 maxReducers=999 totalInputFileSize=120109135\n",
      "2018-02-17 19:11:52,490 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting Parallelism to 1\n",
      "2018-02-17 19:11:52,490 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "2018-02-17 19:11:52,515 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting up single store job\n",
      "2018-02-17 19:11:52,528 [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Key [pig.schematuple] is false, will not generate code.\n",
      "2018-02-17 19:11:52,529 [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Starting process to move generated code to distributed cache\n",
      "2018-02-17 19:11:52,529 [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Distributed cache not supported or needed in local mode. Setting key [pig.schematuple.local.dir] with code temp directory: /tmp/1518894712528-0\n",
      "2018-02-17 19:11:52,645 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 1 map-reduce job(s) waiting for submission.\n",
      "2018-02-17 19:11:52,646 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.tracker.http.address is deprecated. Instead, use mapreduce.jobtracker.http.address\n",
      "2018-02-17 19:11:52,658 [JobControl] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "2018-02-17 19:11:52,870 [JobControl] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n",
      "2018-02-17 19:11:52,919 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths to process : 1\n",
      "2018-02-17 19:11:52,919 [JobControl] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths to process : 1\n",
      "2018-02-17 19:11:52,973 [JobControl] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths (combined) to process : 4\n",
      "2018-02-17 19:11:53,034 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:4\n",
      "2018-02-17 19:11:53,054 [JobControl] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-17 19:11:53,238 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_local864356416_0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-17 19:11:53,522 [JobControl] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://localhost:8080/\n",
      "2018-02-17 19:11:53,527 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - HadoopJobId: job_local864356416_0001\n",
      "2018-02-17 19:11:53,527 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Processing aliases 1-1,cleandata,cleandata_filtered,data,index,pairs,sorted_bag,sorted_index,word_groups,words_data,words_data_filtered\n",
      "2018-02-17 19:11:53,528 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - detailed locations: M: data[8,4],cleandata[-1,-1],cleandata_filtered[25,21],words_data[28,13],words_data_filtered[29,22],word_groups[32,14] C:  R: index[35,8],1-1[36,21],pairs[36,12],1-1[36,21],pairs[36,12],sorted_index[42,15],sorted_bag[43,17]\n",
      "2018-02-17 19:11:53,534 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 0% complete\n",
      "2018-02-17 19:11:53,539 [Thread-5] INFO  org.apache.hadoop.mapred.LocalJobRunner - OutputCommitter set in config null\n",
      "2018-02-17 19:11:53,591 [Thread-5] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.textoutputformat.separator is deprecated. Instead, use mapreduce.output.textoutputformat.separator\n",
      "2018-02-17 19:11:53,593 [Thread-5] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "2018-02-17 19:11:53,594 [Thread-5] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.reduce.markreset.buffer.percent is deprecated. Instead, use mapreduce.reduce.markreset.buffer.percent\n",
      "2018-02-17 19:11:53,595 [Thread-5] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "2018-02-17 19:11:53,595 [Thread-5] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-17 19:11:53,597 [Thread-5] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "2018-02-17 19:11:53,599 [Thread-5] INFO  org.apache.hadoop.mapred.LocalJobRunner - OutputCommitter is org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter\n",
      "2018-02-17 19:11:53,671 [Thread-5] INFO  org.apache.hadoop.mapred.LocalJobRunner - Waiting for map tasks\n",
      "2018-02-17 19:11:53,673 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Starting task: attempt_local864356416_0001_m_000000_0\n",
      "2018-02-17 19:11:53,766 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "2018-02-17 19:11:53,815 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task -  Using ResourceCalculatorProcessTree : [ ]\n",
      "2018-02-17 19:11:53,831 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Processing split: Number of splits :1\n",
      "Total Length = 33554432\n",
      "Input split[0]:\n",
      "   Length = 33554432\n",
      "  Locations:\n",
      "\n",
      "-----------------------\n",
      "\n",
      "2018-02-17 19:11:53,860 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader - Current split being processed file:/media/notebooks/pig-indiceinvertido/forum_node.tsv:0+33554432\n",
      "2018-02-17 19:11:53,940 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - (EQUATOR) 0 kvi 26214396(104857584)\n",
      "2018-02-17 19:11:53,940 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - mapreduce.task.io.sort.mb: 100\n",
      "2018-02-17 19:11:53,940 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - soft limit at 83886080\n",
      "2018-02-17 19:11:53,940 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 0; bufvoid = 104857600\n",
      "2018-02-17 19:11:53,940 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 26214396; length = 6553600\n",
      "2018-02-17 19:11:53,948 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "2018-02-17 19:11:53,989 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.data.SchemaTupleBackend - Key [pig.schematuple] was not set... will not generate code.\n",
      "2018-02-17 19:11:54,015 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Map - Aliases being processed per job phase (AliasName[line,offset]): M: data[8,4],cleandata[-1,-1],cleandata_filtered[25,21],words_data[28,13],words_data_filtered[29,22],word_groups[32,14] C:  R: index[35,8],1-1[36,21],pairs[36,12],1-1[36,21],pairs[36,12],sorted_index[42,15],sorted_bag[43,17]\n",
      "2018-02-17 19:11:59,776 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map\n",
      "2018-02-17 19:12:01,895 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Spilling map output\n",
      "2018-02-17 19:12:01,895 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 0; bufend = 39858232; bufvoid = 104857600\n",
      "2018-02-17 19:12:01,895 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 26214396(104857584); kvend = 15207436(60829744); length = 11006961/6553600\n",
      "2018-02-17 19:12:01,895 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - (EQUATOR) 50343980 kvi 12585988(50343952)\n",
      "2018-02-17 19:12:02,778 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map\n",
      "2018-02-17 19:12:05,636 [SpillThread] INFO  org.apache.hadoop.mapred.MapTask - Finished spill 0\n",
      "2018-02-17 19:12:05,637 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - (RESET) equator 50343980 kv 12585988(50343952) kvi 9964564(39858256)\n",
      "2018-02-17 19:12:05,788 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map\n",
      "2018-02-17 19:12:06,183 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map\n",
      "2018-02-17 19:12:06,184 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Starting flush of map output\n",
      "2018-02-17 19:12:06,184 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Spilling map output\n",
      "2018-02-17 19:12:06,184 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 50343980; bufend = 63284165; bufvoid = 104857600\n",
      "2018-02-17 19:12:06,184 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 12585988(50343952); kvend = 9125100(36500400); length = 3460889/6553600\n",
      "2018-02-17 19:12:07,099 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Finished spill 1\n",
      "2018-02-17 19:12:07,108 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Merger - Merging 2 sorted segments\n",
      "2018-02-17 19:12:07,118 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Merger - Down to the last merge-pass, with 2 segments left of total size: 60032390 bytes\n",
      "2018-02-17 19:12:08,790 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > sort\n",
      "2018-02-17 19:12:08,800 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Task:attempt_local864356416_0001_m_000000_0 is done. And is in the process of committing\n",
      "2018-02-17 19:12:08,802 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > sort\n",
      "2018-02-17 19:12:08,802 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Task 'attempt_local864356416_0001_m_000000_0' done.\n",
      "2018-02-17 19:12:08,802 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Finishing task: attempt_local864356416_0001_m_000000_0\n",
      "2018-02-17 19:12:08,802 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Starting task: attempt_local864356416_0001_m_000001_0\n",
      "2018-02-17 19:12:08,814 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "2018-02-17 19:12:08,821 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task -  Using ResourceCalculatorProcessTree : [ ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-17 19:12:08,825 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Processing split: Number of splits :1\n",
      "Total Length = 33554432\n",
      "Input split[0]:\n",
      "   Length = 33554432\n",
      "  Locations:\n",
      "\n",
      "-----------------------\n",
      "\n",
      "2018-02-17 19:12:08,830 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader - Current split being processed file:/media/notebooks/pig-indiceinvertido/forum_node.tsv:33554432+33554432\n",
      "2018-02-17 19:12:08,849 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - (EQUATOR) 0 kvi 26214396(104857584)\n",
      "2018-02-17 19:12:08,849 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - mapreduce.task.io.sort.mb: 100\n",
      "2018-02-17 19:12:08,849 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - soft limit at 83886080\n",
      "2018-02-17 19:12:08,849 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 0; bufvoid = 104857600\n",
      "2018-02-17 19:12:08,849 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 26214396; length = 6553600\n",
      "2018-02-17 19:12:08,850 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "2018-02-17 19:12:08,882 [LocalJobRunner Map Task Executor #0] WARN  org.apache.pig.data.SchemaTupleBackend - SchemaTupleBackend has already been initialized\n",
      "2018-02-17 19:12:08,910 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Map - Aliases being processed per job phase (AliasName[line,offset]): M: data[8,4],cleandata[-1,-1],cleandata_filtered[25,21],words_data[28,13],words_data_filtered[29,22],word_groups[32,14] C:  R: index[35,8],1-1[36,21],pairs[36,12],1-1[36,21],pairs[36,12],sorted_index[42,15],sorted_bag[43,17]\n",
      "2018-02-17 19:12:14,810 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map\n",
      "2018-02-17 19:12:16,112 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Spilling map output\n",
      "2018-02-17 19:12:16,112 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 0; bufend = 40782076; bufvoid = 104857600\n",
      "2018-02-17 19:12:16,112 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 26214396(104857584); kvend = 15438396(61753584); length = 10776001/6553600\n",
      "2018-02-17 19:12:16,113 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - (EQUATOR) 51267822 kvi 12816948(51267792)\n",
      "2018-02-17 19:12:17,810 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map\n",
      "2018-02-17 19:12:19,687 [SpillThread] INFO  org.apache.hadoop.mapred.MapTask - Finished spill 0\n",
      "2018-02-17 19:12:19,687 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - (RESET) equator 51267822 kv 12816948(51267792) kvi 10195524(40782096)\n",
      "2018-02-17 19:12:20,811 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map\n",
      "2018-02-17 19:12:21,454 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map\n",
      "2018-02-17 19:12:21,455 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Starting flush of map output\n",
      "2018-02-17 19:12:21,459 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Spilling map output\n",
      "2018-02-17 19:12:21,459 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 51267822; bufend = 69773215; bufvoid = 104857600\n",
      "2018-02-17 19:12:21,459 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 12816948(51267792); kvend = 7966940(31867760); length = 4850009/6553600\n",
      "2018-02-17 19:12:23,291 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Finished spill 1\n",
      "2018-02-17 19:12:23,302 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Merger - Merging 2 sorted segments\n",
      "2018-02-17 19:12:23,303 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Merger - Down to the last merge-pass, with 2 segments left of total size: 67100520 bytes\n",
      "2018-02-17 19:12:23,815 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > sort > \n",
      "2018-02-17 19:12:26,612 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Task:attempt_local864356416_0001_m_000001_0 is done. And is in the process of committing\n",
      "2018-02-17 19:12:26,614 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > sort\n",
      "2018-02-17 19:12:26,614 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Task 'attempt_local864356416_0001_m_000001_0' done.\n",
      "2018-02-17 19:12:26,614 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Finishing task: attempt_local864356416_0001_m_000001_0\n",
      "2018-02-17 19:12:26,614 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Starting task: attempt_local864356416_0001_m_000002_0\n",
      "2018-02-17 19:12:26,622 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "2018-02-17 19:12:26,623 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task -  Using ResourceCalculatorProcessTree : [ ]\n",
      "2018-02-17 19:12:26,626 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Processing split: Number of splits :1\n",
      "Total Length = 33554432\n",
      "Input split[0]:\n",
      "   Length = 33554432\n",
      "  Locations:\n",
      "\n",
      "-----------------------\n",
      "\n",
      "2018-02-17 19:12:26,633 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader - Current split being processed file:/media/notebooks/pig-indiceinvertido/forum_node.tsv:67108864+33554432\n",
      "2018-02-17 19:12:26,649 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - (EQUATOR) 0 kvi 26214396(104857584)\n",
      "2018-02-17 19:12:26,650 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - mapreduce.task.io.sort.mb: 100\n",
      "2018-02-17 19:12:26,650 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - soft limit at 83886080\n",
      "2018-02-17 19:12:26,651 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 0; bufvoid = 104857600\n",
      "2018-02-17 19:12:26,651 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 26214396; length = 6553600\n",
      "2018-02-17 19:12:26,652 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "2018-02-17 19:12:26,814 [LocalJobRunner Map Task Executor #0] WARN  org.apache.pig.data.SchemaTupleBackend - SchemaTupleBackend has already been initialized\n",
      "2018-02-17 19:12:26,861 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Map - Aliases being processed per job phase (AliasName[line,offset]): M: data[8,4],cleandata[-1,-1],cleandata_filtered[25,21],words_data[28,13],words_data_filtered[29,22],word_groups[32,14] C:  R: index[35,8],1-1[36,21],pairs[36,12],1-1[36,21],pairs[36,12],sorted_index[42,15],sorted_bag[43,17]\n",
      "2018-02-17 19:12:32,649 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map\n",
      "2018-02-17 19:12:35,652 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map\n",
      "2018-02-17 19:12:36,593 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Spilling map output\n",
      "2018-02-17 19:12:36,593 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 0; bufend = 40677656; bufvoid = 104857600\n",
      "2018-02-17 19:12:36,593 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 26214396(104857584); kvend = 15412296(61649184); length = 10802101/6553600\n",
      "2018-02-17 19:12:36,593 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - (EQUATOR) 51163412 kvi 12790848(51163392)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-17 19:12:38,653 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map\n",
      "2018-02-17 19:12:40,234 [SpillThread] INFO  org.apache.hadoop.mapred.MapTask - Finished spill 0\n",
      "2018-02-17 19:12:40,235 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - (RESET) equator 51163412 kv 12790848(51163392) kvi 10169420(40677680)\n",
      "2018-02-17 19:12:41,655 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map\n",
      "2018-02-17 19:12:41,886 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map\n",
      "2018-02-17 19:12:41,887 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Starting flush of map output\n",
      "2018-02-17 19:12:41,890 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Spilling map output\n",
      "2018-02-17 19:12:41,890 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 51163412; bufend = 70737927; bufvoid = 104857600\n",
      "2018-02-17 19:12:41,890 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 12790848(51163392); kvend = 7585048(30340192); length = 5205801/6553600\n",
      "2018-02-17 19:12:43,356 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Finished spill 1\n",
      "2018-02-17 19:12:43,358 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Merger - Merging 2 sorted segments\n",
      "2018-02-17 19:12:43,359 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Merger - Down to the last merge-pass, with 2 segments left of total size: 68256169 bytes\n",
      "2018-02-17 19:12:44,657 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > sort > \n",
      "2018-02-17 19:12:45,282 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Task:attempt_local864356416_0001_m_000002_0 is done. And is in the process of committing\n",
      "2018-02-17 19:12:45,285 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > sort\n",
      "2018-02-17 19:12:45,285 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Task 'attempt_local864356416_0001_m_000002_0' done.\n",
      "2018-02-17 19:12:45,285 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Finishing task: attempt_local864356416_0001_m_000002_0\n",
      "2018-02-17 19:12:45,285 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Starting task: attempt_local864356416_0001_m_000003_0\n",
      "2018-02-17 19:12:45,289 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "2018-02-17 19:12:45,289 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task -  Using ResourceCalculatorProcessTree : [ ]\n",
      "2018-02-17 19:12:45,290 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Processing split: Number of splits :1\n",
      "Total Length = 19445839\n",
      "Input split[0]:\n",
      "   Length = 19445839\n",
      "  Locations:\n",
      "\n",
      "-----------------------\n",
      "\n",
      "2018-02-17 19:12:45,292 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader - Current split being processed file:/media/notebooks/pig-indiceinvertido/forum_node.tsv:100663296+19445839\n",
      "2018-02-17 19:12:45,309 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - (EQUATOR) 0 kvi 26214396(104857584)\n",
      "2018-02-17 19:12:45,309 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - mapreduce.task.io.sort.mb: 100\n",
      "2018-02-17 19:12:45,309 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - soft limit at 83886080\n",
      "2018-02-17 19:12:45,309 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 0; bufvoid = 104857600\n",
      "2018-02-17 19:12:45,309 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 26214396; length = 6553600\n",
      "2018-02-17 19:12:45,311 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "2018-02-17 19:12:45,327 [LocalJobRunner Map Task Executor #0] WARN  org.apache.pig.data.SchemaTupleBackend - SchemaTupleBackend has already been initialized\n",
      "2018-02-17 19:12:45,335 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Map - Aliases being processed per job phase (AliasName[line,offset]): M: data[8,4],cleandata[-1,-1],cleandata_filtered[25,21],words_data[28,13],words_data_filtered[29,22],word_groups[32,14] C:  R: index[35,8],1-1[36,21],pairs[36,12],1-1[36,21],pairs[36,12],sorted_index[42,15],sorted_bag[43,17]\n",
      "2018-02-17 19:12:51,293 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map\n",
      "2018-02-17 19:12:52,636 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map\n",
      "2018-02-17 19:12:52,637 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Starting flush of map output\n",
      "2018-02-17 19:12:52,637 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Spilling map output\n",
      "2018-02-17 19:12:52,637 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 0; bufend = 40451886; bufvoid = 104857600\n",
      "2018-02-17 19:12:52,637 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 26214396(104857584); kvend = 15404404(61617616); length = 10809993/6553600\n",
      "2018-02-17 19:12:54,298 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > sort\n",
      "2018-02-17 19:12:56,127 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Finished spill 0\n",
      "2018-02-17 19:12:56,128 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Task:attempt_local864356416_0001_m_000003_0 is done. And is in the process of committing\n",
      "2018-02-17 19:12:56,129 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - map\n",
      "2018-02-17 19:12:56,130 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Task 'attempt_local864356416_0001_m_000003_0' done.\n",
      "2018-02-17 19:12:56,130 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Finishing task: attempt_local864356416_0001_m_000003_0\n",
      "2018-02-17 19:12:56,130 [Thread-5] INFO  org.apache.hadoop.mapred.LocalJobRunner - map task executor complete.\n",
      "2018-02-17 19:12:56,144 [Thread-5] INFO  org.apache.hadoop.mapred.LocalJobRunner - Waiting for reduce tasks\n",
      "2018-02-17 19:12:56,144 [pool-5-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - Starting task: attempt_local864356416_0001_r_000000_0\n",
      "2018-02-17 19:12:56,174 [pool-5-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "2018-02-17 19:12:56,176 [pool-5-thread-1] INFO  org.apache.hadoop.mapred.Task -  Using ResourceCalculatorProcessTree : [ ]\n",
      "2018-02-17 19:12:56,187 [pool-5-thread-1] INFO  org.apache.hadoop.mapred.ReduceTask - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6435a194\n",
      "2018-02-17 19:12:56,218 [pool-5-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - MergerManager: memoryLimit=652528832, maxSingleShuffleLimit=163132208, mergeThreshold=430669056, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "2018-02-17 19:12:56,222 [EventFetcher for fetching Map Completion Events] INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher - attempt_local864356416_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "2018-02-17 19:12:56,320 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.LocalFetcher - localfetcher#1 about to shuffle output of map attempt_local864356416_0001_m_000001_0 decomp: 67100522 len: 67100526 to MEMORY\n",
      "2018-02-17 19:12:56,444 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput - Read 67100522 bytes from map-output for attempt_local864356416_0001_m_000001_0\n",
      "2018-02-17 19:12:56,453 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - closeInMemoryFile -> map-output of size: 67100522, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->67100522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-17 19:12:56,476 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.LocalFetcher - localfetcher#1 about to shuffle output of map attempt_local864356416_0001_m_000000_0 decomp: 60032392 len: 60032396 to MEMORY\n",
      "2018-02-17 19:12:56,648 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput - Read 60032392 bytes from map-output for attempt_local864356416_0001_m_000000_0\n",
      "2018-02-17 19:12:56,648 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - closeInMemoryFile -> map-output of size: 60032392, inMemoryMapOutputs.size() -> 2, commitMemory -> 67100522, usedMemory ->127132914\n",
      "2018-02-17 19:12:56,656 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.LocalFetcher - localfetcher#1 about to shuffle output of map attempt_local864356416_0001_m_000003_0 decomp: 45856899 len: 45856903 to MEMORY\n",
      "2018-02-17 19:12:56,726 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput - Read 45856899 bytes from map-output for attempt_local864356416_0001_m_000003_0\n",
      "2018-02-17 19:12:56,726 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - closeInMemoryFile -> map-output of size: 45856899, inMemoryMapOutputs.size() -> 3, commitMemory -> 127132914, usedMemory ->172989813\n",
      "2018-02-17 19:12:56,745 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.LocalFetcher - localfetcher#1 about to shuffle output of map attempt_local864356416_0001_m_000002_0 decomp: 68256171 len: 68256175 to MEMORY\n",
      "2018-02-17 19:12:56,848 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput - Read 68256171 bytes from map-output for attempt_local864356416_0001_m_000002_0\n",
      "2018-02-17 19:12:56,848 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - closeInMemoryFile -> map-output of size: 68256171, inMemoryMapOutputs.size() -> 4, commitMemory -> 172989813, usedMemory ->241245984\n",
      "2018-02-17 19:12:56,849 [EventFetcher for fetching Map Completion Events] INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher - EventFetcher is interrupted.. Returning\n",
      "2018-02-17 19:12:56,851 [pool-5-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - 4 / 4 copied.\n",
      "2018-02-17 19:12:56,851 [pool-5-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - finalMerge called with 4 in-memory map-outputs and 0 on-disk map-outputs\n",
      "2018-02-17 19:12:56,854 [pool-5-thread-1] INFO  org.apache.hadoop.mapred.Merger - Merging 4 sorted segments\n",
      "2018-02-17 19:12:56,856 [pool-5-thread-1] INFO  org.apache.hadoop.mapred.Merger - Down to the last merge-pass, with 4 segments left of total size: 241245960 bytes\n",
      "2018-02-17 19:13:02,107 [pool-5-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - Merged 4 segments, 241245984 bytes to disk to satisfy reduce memory limit\n",
      "2018-02-17 19:13:02,108 [pool-5-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - Merging 1 files, 241245982 bytes from disk\n",
      "2018-02-17 19:13:02,110 [pool-5-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - Merging 0 segments, 0 bytes from memory into reduce\n",
      "2018-02-17 19:13:02,110 [pool-5-thread-1] INFO  org.apache.hadoop.mapred.Merger - Merging 1 sorted segments\n",
      "2018-02-17 19:13:02,110 [pool-5-thread-1] INFO  org.apache.hadoop.mapred.Merger - Down to the last merge-pass, with 1 segments left of total size: 241245972 bytes\n",
      "2018-02-17 19:13:02,111 [pool-5-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - 4 / 4 copied.\n",
      "2018-02-17 19:13:02,133 [pool-5-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "2018-02-17 19:13:02,154 [pool-5-thread-1] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "2018-02-17 19:13:02,164 [pool-5-thread-1] WARN  org.apache.pig.data.SchemaTupleBackend - SchemaTupleBackend has already been initialized\n",
      "2018-02-17 19:13:02,175 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-17 19:13:02,193 [pool-5-thread-1] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce - Aliases being processed per job phase (AliasName[line,offset]): M: data[8,4],cleandata[-1,-1],cleandata_filtered[25,21],words_data[28,13],words_data_filtered[29,22],word_groups[32,14] C:  R: index[35,8],1-1[36,21],pairs[36,12],1-1[36,21],pairs[36,12],sorted_index[42,15],sorted_bag[43,17]\n",
      "2018-02-17 19:13:05,177 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-17 19:13:08,180 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-17 19:13:11,181 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-17 19:13:14,181 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-17 19:13:17,331 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-17 19:13:20,333 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-17 19:13:23,334 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-17 19:13:26,337 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-17 19:13:29,341 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-17 19:13:32,343 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-17 19:13:35,344 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-17 19:13:38,348 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-17 19:13:41,350 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-17 19:13:44,353 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-17 19:13:47,357 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-17 19:13:50,362 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-17 19:13:53,362 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-17 19:13:56,364 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-17 19:13:59,368 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-17 19:14:02,378 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-17 19:14:05,380 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-17 19:14:08,380 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-17 19:14:11,381 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-17 19:14:14,386 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-17 19:14:17,390 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-17 19:14:20,391 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-17 19:14:23,394 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-17 19:14:26,396 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-17 19:14:29,399 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-17 19:14:32,400 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-17 19:14:35,401 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-17 19:14:36,557 [pool-5-thread-1] INFO  org.apache.hadoop.mapred.Task - Task:attempt_local864356416_0001_r_000000_0 is done. And is in the process of committing\n",
      "2018-02-17 19:14:36,644 [pool-5-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-17 19:14:36,645 [pool-5-thread-1] INFO  org.apache.hadoop.mapred.Task - Task attempt_local864356416_0001_r_000000_0 is allowed to commit now\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-17 19:14:36,654 [pool-5-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_local864356416_0001_r_000000_0' to file:/media/notebooks/pig-indiceinvertido/inverted_index/_temporary/0/task_local864356416_0001_r_000000\n",
      "2018-02-17 19:14:36,655 [pool-5-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-17 19:14:36,655 [pool-5-thread-1] INFO  org.apache.hadoop.mapred.Task - Task 'attempt_local864356416_0001_r_000000_0' done.\n",
      "2018-02-17 19:14:36,655 [pool-5-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - Finishing task: attempt_local864356416_0001_r_000000_0\n",
      "2018-02-17 19:14:36,655 [Thread-5] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce task executor complete.\n",
      "2018-02-17 19:14:37,272 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "2018-02-17 19:14:37,272 [main] WARN  org.apache.pig.tools.pigstats.PigStatsUtil - Failed to get RunningJob for job job_local864356416_0001\n",
      "2018-02-17 19:14:37,279 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 100% complete\n",
      "2018-02-17 19:14:37,279 [main] INFO  org.apache.pig.tools.pigstats.SimplePigStats - Detected Local mode. Stats reported below may be incomplete\n",
      "2018-02-17 19:14:37,286 [main] INFO  org.apache.pig.tools.pigstats.SimplePigStats - Script Statistics: \n",
      "\n",
      "HadoopVersion\tPigVersion\tUserId\tStartedAt\tFinishedAt\tFeatures\n",
      "2.6.0-cdh5.9.0\t0.12.0-cdh5.9.0\troot\t2018-02-17 19:11:52\t2018-02-17 19:14:37\tGROUP_BY,FILTER\n",
      "\n",
      "Success!\n",
      "\n",
      "Job Stats (time in seconds):\n",
      "JobId\tAlias\tFeature\tOutputs\n",
      "job_local864356416_0001\t1-1,cleandata,cleandata_filtered,data,index,pairs,sorted_bag,sorted_index,word_groups,words_data,words_data_filtered\tGROUP_BY,DISTINCT\tfile:///media/notebooks/pig-indiceinvertido/inverted_index,\n",
      "\n",
      "Input(s):\n",
      "Successfully read records from: \"file:///media/notebooks/pig-indiceinvertido/forum_node.tsv\"\n",
      "\n",
      "Output(s):\n",
      "Successfully stored records in: \"file:///media/notebooks/pig-indiceinvertido/inverted_index\"\n",
      "\n",
      "Job DAG:\n",
      "job_local864356416_0001\n",
      "\n",
      "\n",
      "2018-02-17 19:14:37,287 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Success!\n"
     ]
    }
   ],
   "source": [
    "! pig -f students-inverted-index.pig -x local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t{(2010230)}\t1\r\n",
      "reciprocals\t{(2009620),(2009707),(2010947),(2014292),(9002395)}\t5\r\n",
      "reciprocate\t{(2001902),(6004447)}\t2\r\n",
      "reciprocity\t{(2008341),(2009563),(2009571),(2009804),(2010292)}\t5\r\n",
      "recitations\t{(2018236),(5014249)}\t2\r\n",
      "reclamation\t{(10438)}\t1\r\n",
      "recliningon\t{(6022210)}\t1\r\n",
      "recognisers\t{(5007198),(5007700),(5007756)}\t3\r\n",
      "recognising\t{(1014977),(1034000),(6015555),(8000513)}\t4\r\n",
      "recognition\t{(3982),(5301),(8066),(9102),(22789),(41152),(47260),(51450),(51559),(52330),(53802),(53881),(53962),(60657),(60916),(63420),(64321),(64470),(66804),(66874),(67092),(67121),(67242),(67488),(1000219),(1000901),(1001733),(1001920),(1002371),(1005129),(1006927),(1007272),(1008146),(1008955),(1008996),(1009010),(1009776),(1010107),(1010329),(1010351),(1012085),(1012836),(1013518),(1013848),(1014107),(1015692),(1018390),(1023592),(1025649),(1026946),(1028196),(1030214),(1030460),(1030646),(1030651),(1031191),(1031238),(1031734),(1032697),(1032720),(1033371),(1033635),(1033857),(1034081),(1034451),(1034946),(1034954),(1035052),(1035056),(1035061),(1035176),(1035289),(2001357),(2006024),(2013591),(2016523),(2016895),(3000200),(3000463),(3001489),(3001492),(3001645),(3001837),(5004737),(5011243),(5014578),(5014587),(6000004),(6021164),(6031048),(7002749),(7003833),(7007679),(8001664),(8002598),(8004135),(8005177),(10000159),(10000366),(10000718),(10001470),(10002665),(10003051),(10003285),(10003312),(10005751),(10006251),(10006449),(10006944),(10007030),(10007571),(10007850),(10010554),(10010641),(10011519),(12003284),(12003362)}\t117\r\n",
      "tail: write error: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "! tail -40000 ./inverted_index/part-r-00000  | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ejecución del fichero PIG en Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log4j:WARN No appenders could be found for logger (org.apache.hadoop.util.Shell).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n",
      "2018-02-17 20:12:23,409 [main] INFO  org.apache.pig.Main - Apache Pig version 0.12.0-cdh5.9.0 (rUnversioned directory) compiled Oct 21 2016, 01:17:18\n",
      "2018-02-17 20:12:23,410 [main] INFO  org.apache.pig.Main - Logging error messages to: /media/notebooks/pig-indiceinvertido/pig-indiceinvertido/pig-indiceinvertido/pig_1518898343373.log\n",
      "2018-02-17 20:12:24,649 [main] ERROR org.apache.pig.Main - ERROR 2997: Encountered IOException. File students-inverted-index.pig does not exist\n",
      "Details at logfile: /media/notebooks/pig-indiceinvertido/pig-indiceinvertido/pig-indiceinvertido/pig_1518898343373.log\n"
     ]
    }
   ],
   "source": [
    "! pig -f students-inverted-index.pig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   3 root supergroup          0 2018-02-05 15:42 inverted_index/_SUCCESS\r\n",
      "-rw-r--r--   3 root supergroup   86936662 2018-02-05 15:42 inverted_index/part-r-00000\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls inverted_index/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t{(2010230)}\t1\r\n",
      "reciprocals\t{(2009620),(2009707),(2010947),(2014292),(9002395)}\t5\r\n",
      "reciprocate\t{(2001902),(6004447)}\t2\r\n",
      "reciprocity\t{(2008341),(2009563),(2009571),(2009804),(2010292)}\t5\r\n",
      "recitations\t{(2018236),(5014249)}\t2\r\n",
      "reclamation\t{(10438)}\t1\r\n",
      "recliningon\t{(6022210)}\t1\r\n",
      "recognisers\t{(5007198),(5007700),(5007756)}\t3\r\n",
      "recognising\t{(1014977),(1034000),(6015555),(8000513)}\t4\r\n",
      "recognition\t{(3982),(5301),(8066),(9102),(22789),(41152),(47260),(51450),(51559),(52330),(53802),(53881),(53962),(60657),(60916),(63420),(64321),(64470),(66804),(66874),(67092),(67121),(67242),(67488),(1000219),(1000901),(1001733),(1001920),(1002371),(1005129),(1006927),(1007272),(1008146),(1008955),(1008996),(1009010),(1009776),(1010107),(1010329),(1010351),(1012085),(1012836),(1013518),(1013848),(1014107),(1015692),(1018390),(1023592),(1025649),(1026946),(1028196),(1030214),(1030460),(1030646),(1030651),(1031191),(1031238),(1031734),(1032697),(1032720),(1033371),(1033635),(1033857),(1034081),(1034451),(1034946),(1034954),(1035052),(1035056),(1035061),(1035176),(1035289),(2001357),(2006024),(2013591),(2016523),(2016895),(3000200),(3000463),(3001489),(3001492),(3001645),(3001837),(5004737),(5011243),(5014578),(5014587),(6000004),(6021164),(6031048),(7002749),(7003833),(7007679),(8001664),(8002598),(8004135),(8005177),(10000159),(10000366),(10000718),(10001470),(10002665),(10003051),(10003285),(10003312),(10005751),(10006251),(10006449),(10006944),(10007030),(10007571),(10007850),(10010554),(10010641),(10011519),(12003284),(12003362)}\t117\r\n",
      "tail: write error: Broken pipe\r\n",
      "tail: write error\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -cat inverted_index/part-r-00000 | tail -40000 | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
