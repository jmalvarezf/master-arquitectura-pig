{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pig: Índice invertido\n",
    "\n",
    "Partiendo del dataset de posts utilizado anteriormente, vamos a calcular un índice invertido.\n",
    "\n",
    "Este notebook se ejecutará dentro del cluster de Hadoop creado usando Docker durante las clases del máster. En concreto, se usarán las siguientes imágenes:\n",
    "* accaminero/namenode01\n",
    "* swapnillinux/cloudera-hadoop-yarnmaster\n",
    "* swapnillinux/cloudera-hadoop-datanode\n",
    "\n",
    "Se puede construir el cluster Hadoop y lanzar el entorno Jupyter usando los siguientes scripts incluídos en la práctica:\n",
    "* docker/build.sh\n",
    "* docker/jupyter.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparación del entorno de ejecución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una carpeta local para almacenar los ficheros de trabajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/notebooks/pig-indiceinvertido\r\n"
     ]
    }
   ],
   "source": [
    "! rm -fr pig-indiceinvertido\n",
    "! mkdir -p pig-indiceinvertido\n",
    "import os\n",
    "os.chdir(\"pig-indiceinvertido/\")\n",
    "! pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instalación de depencias:\n",
    "* Instalamos dos2unix para limpiar el fichero y convertirlo de formato DOS a Unix\n",
    "* Instalamos pig para ejecutar los correspondientes comandos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded plugins: fastestmirror, ovl\n",
      "Repodata is over 2 weeks old. Install yum-cron? Or run: yum makecache fast\n",
      "base                                                     | 3.6 kB     00:00     \n",
      "cloudera-cdh                                             |  951 B     00:00     \n",
      "epel/x86_64/metalink                                     |  24 kB     00:00     \n",
      "epel                                                     | 4.7 kB     00:00     \n",
      "http://ftp.cica.es/epel/7/x86_64/repodata/repomd.xml: [Errno -1] repomd.xml does not match metalink for epel\n",
      "Trying other mirror.\n",
      "epel                                                     | 4.7 kB     00:00     \n",
      "extras                                                   | 3.4 kB     00:00     \n",
      "updates                                                  | 3.4 kB     00:00     \n",
      "(1/5): extras/7/x86_64/primary_db                          | 166 kB   00:00     \n",
      "(2/5): epel/x86_64/group_gz                                | 266 kB   00:01     \n",
      "(3/5): epel/x86_64/updateinfo                              | 882 kB   00:02     \n",
      "(4/5): epel/x86_64/primary_db                              | 6.2 MB   00:05     \n",
      "(5/5): updates/7/x86_64/primary_db                         | 6.0 MB   00:07     \n",
      "Determining fastest mirrors\n",
      " * base: ftp.rediris.es\n",
      " * epel: ftp.cica.es\n",
      " * extras: ftp.rediris.es\n",
      " * updates: ftp.rediris.es\n",
      "Resolving Dependencies\n",
      "--> Running transaction check\n",
      "---> Package dos2unix.x86_64 0:6.0.3-7.el7 will be installed\n",
      "---> Package hbase.x86_64 0:1.2.0+cdh5.9.0+205-1.cdh5.9.0.p0.30.el7 will be installed\n",
      "---> Package pig.noarch 0:0.12.0+cdh5.9.0+95-1.cdh5.9.0.p0.30.el7 will be installed\n",
      "--> Processing Dependency: hive-hcatalog for package: pig-0.12.0+cdh5.9.0+95-1.cdh5.9.0.p0.30.el7.noarch\n",
      "--> Processing Dependency: hive for package: pig-0.12.0+cdh5.9.0+95-1.cdh5.9.0.p0.30.el7.noarch\n",
      "--> Running transaction check\n",
      "---> Package hive.noarch 0:1.1.0+cdh5.9.0+752-1.cdh5.9.0.p0.30.el7 will be installed\n",
      "--> Processing Dependency: hive-jdbc = 1.1.0+cdh5.9.0+752-1.cdh5.9.0.p0.30.el7 for package: hive-1.1.0+cdh5.9.0+752-1.cdh5.9.0.p0.30.el7.noarch\n",
      "--> Processing Dependency: sentry for package: hive-1.1.0+cdh5.9.0+752-1.cdh5.9.0.p0.30.el7.noarch\n",
      "---> Package hive-hcatalog.noarch 0:1.1.0+cdh5.9.0+752-1.cdh5.9.0.p0.30.el7 will be installed\n",
      "--> Running transaction check\n",
      "---> Package hive-jdbc.noarch 0:1.1.0+cdh5.9.0+752-1.cdh5.9.0.p0.30.el7 will be installed\n",
      "---> Package sentry.noarch 0:1.5.1+cdh5.9.0+261-1.cdh5.9.0.p0.30.el7 will be installed\n",
      "--> Processing Dependency: solr >= 4.10.3+cdh5.5.0 for package: sentry-1.5.1+cdh5.9.0+261-1.cdh5.9.0.p0.30.el7.noarch\n",
      "--> Running transaction check\n",
      "---> Package solr.noarch 0:4.10.3+cdh5.9.0+458-1.cdh5.9.0.p0.30.el7 will be installed\n",
      "base/7/x86_64/filelists_db                               | 6.7 MB     00:03     \n",
      "cloudera-cdh/filelists                                   | 405 kB     00:00     \n",
      "--> Processing Dependency: bigtop-tomcat for package: solr-4.10.3+cdh5.9.0+458-1.cdh5.9.0.p0.30.el7.noarch\n",
      "--> Running transaction check\n",
      "---> Package bigtop-tomcat.noarch 0:0.7.0+cdh5.9.0+0-1.cdh5.9.0.p0.30.el7 will be installed\n",
      "--> Finished Dependency Resolution\n",
      "\n",
      "Dependencies Resolved\n",
      "\n",
      "================================================================================\n",
      " Package     Arch   Version                                  Repository    Size\n",
      "================================================================================\n",
      "Installing:\n",
      " dos2unix    x86_64 6.0.3-7.el7                              base          74 k\n",
      " hbase       x86_64 1.2.0+cdh5.9.0+205-1.cdh5.9.0.p0.30.el7  cloudera-cdh  62 M\n",
      " pig         noarch 0.12.0+cdh5.9.0+95-1.cdh5.9.0.p0.30.el7  cloudera-cdh  58 M\n",
      "Installing for dependencies:\n",
      " bigtop-tomcat\n",
      "             noarch 0.7.0+cdh5.9.0+0-1.cdh5.9.0.p0.30.el7    cloudera-cdh 7.3 M\n",
      " hive        noarch 1.1.0+cdh5.9.0+752-1.cdh5.9.0.p0.30.el7  cloudera-cdh  40 M\n",
      " hive-hcatalog\n",
      "             noarch 1.1.0+cdh5.9.0+752-1.cdh5.9.0.p0.30.el7  cloudera-cdh 381 k\n",
      " hive-jdbc   noarch 1.1.0+cdh5.9.0+752-1.cdh5.9.0.p0.30.el7  cloudera-cdh  45 M\n",
      " sentry      noarch 1.5.1+cdh5.9.0+261-1.cdh5.9.0.p0.30.el7  cloudera-cdh  54 M\n",
      " solr        noarch 4.10.3+cdh5.9.0+458-1.cdh5.9.0.p0.30.el7 cloudera-cdh  73 M\n",
      "\n",
      "Transaction Summary\n",
      "================================================================================\n",
      "Install  3 Packages (+6 Dependent packages)\n",
      "\n",
      "Total download size: 342 M\n",
      "Installed size: 474 M\n",
      "Downloading packages:\n",
      "(1/9): dos2unix-6.0.3-7.el7.x86_64.rpm                     |  74 kB   00:04     \n",
      "(2/9): bigtop-tomcat-0.7.0+cdh5.9.0+0-1.cdh5.9.0.p0.30.el7 | 7.3 MB   00:05     \n",
      "(3/9): hive-1.1.0+cdh5.9.0+752-1.cdh5.9.0.p0.30.el7.noarch |  40 MB   00:26     \n",
      "(4/9): hive-hcatalog-1.1.0+cdh5.9.0+752-1.cdh5.9.0.p0.30.e | 381 kB   00:00     \n",
      "(5/9): hbase-1.2.0+cdh5.9.0+205-1.cdh5.9.0.p0.30.el7.x86_6 |  62 MB   00:51     \n",
      "(6/9): hive-jdbc-1.1.0+cdh5.9.0+752-1.cdh5.9.0.p0.30.el7.n |  45 MB   00:32     \n",
      "(7/9): pig-0.12.0+cdh5.9.0+95-1.cdh5.9.0.p0.30.el7.noarch. |  58 MB   00:45     \n",
      "(8/9): sentry-1.5.1+cdh5.9.0+261-1.cdh5.9.0.p0.30.el7.noar |  54 MB   00:43     \n",
      "(9/9): solr-4.10.3+cdh5.9.0+458-1.cdh5.9.0.p0.30.el7.noarc |  73 MB   00:33     \n",
      "--------------------------------------------------------------------------------\n",
      "Total                                              2.6 MB/s | 342 MB  02:09     \n",
      "Running transaction check\n",
      "Running transaction test\n",
      "Transaction test succeeded\n",
      "Running transaction\n",
      "  Installing : hive-jdbc-1.1.0+cdh5.9.0+752-1.cdh5.9.0.p0.30.el7.noarch     1/9 \n",
      "  Installing : bigtop-tomcat-0.7.0+cdh5.9.0+0-1.cdh5.9.0.p0.30.el7.noarch   2/9 \n",
      "  Installing : solr-4.10.3+cdh5.9.0+458-1.cdh5.9.0.p0.30.el7.noarch         3/9 \n",
      "The following warning applies to any collections configured to\n",
      "use Non-SolrCloud mode. Any such collection configuration will\n",
      "need to be upgraded, see Upgrading Cloudera Search for details.\n",
      "  Installing : sentry-1.5.1+cdh5.9.0+261-1.cdh5.9.0.p0.30.el7.noarch        4/9 \n",
      "  Installing : hive-1.1.0+cdh5.9.0+752-1.cdh5.9.0.p0.30.el7.noarch          5/9 \n",
      "  Installing : hive-hcatalog-1.1.0+cdh5.9.0+752-1.cdh5.9.0.p0.30.el7.noar   6/9 \n",
      "  Installing : pig-0.12.0+cdh5.9.0+95-1.cdh5.9.0.p0.30.el7.noarch           7/9 \n",
      "  Installing : dos2unix-6.0.3-7.el7.x86_64                                  8/9 \n",
      "  Installing : hbase-1.2.0+cdh5.9.0+205-1.cdh5.9.0.p0.30.el7.x86_64         9/9 \n",
      "  Verifying  : hive-jdbc-1.1.0+cdh5.9.0+752-1.cdh5.9.0.p0.30.el7.noarch     1/9 \n",
      "  Verifying  : hbase-1.2.0+cdh5.9.0+205-1.cdh5.9.0.p0.30.el7.x86_64         2/9 \n",
      "  Verifying  : dos2unix-6.0.3-7.el7.x86_64                                  3/9 \n",
      "  Verifying  : sentry-1.5.1+cdh5.9.0+261-1.cdh5.9.0.p0.30.el7.noarch        4/9 \n",
      "  Verifying  : solr-4.10.3+cdh5.9.0+458-1.cdh5.9.0.p0.30.el7.noarch         5/9 \n",
      "  Verifying  : pig-0.12.0+cdh5.9.0+95-1.cdh5.9.0.p0.30.el7.noarch           6/9 \n",
      "  Verifying  : hive-hcatalog-1.1.0+cdh5.9.0+752-1.cdh5.9.0.p0.30.el7.noar   7/9 \n",
      "  Verifying  : hive-1.1.0+cdh5.9.0+752-1.cdh5.9.0.p0.30.el7.noarch          8/9 \n",
      "  Verifying  : bigtop-tomcat-0.7.0+cdh5.9.0+0-1.cdh5.9.0.p0.30.el7.noarch   9/9 \n",
      "\n",
      "Installed:\n",
      "  dos2unix.x86_64 0:6.0.3-7.el7                                                 \n",
      "  hbase.x86_64 0:1.2.0+cdh5.9.0+205-1.cdh5.9.0.p0.30.el7                        \n",
      "  pig.noarch 0:0.12.0+cdh5.9.0+95-1.cdh5.9.0.p0.30.el7                          \n",
      "\n",
      "Dependency Installed:\n",
      "  bigtop-tomcat.noarch 0:0.7.0+cdh5.9.0+0-1.cdh5.9.0.p0.30.el7                  \n",
      "  hive.noarch 0:1.1.0+cdh5.9.0+752-1.cdh5.9.0.p0.30.el7                         \n",
      "  hive-hcatalog.noarch 0:1.1.0+cdh5.9.0+752-1.cdh5.9.0.p0.30.el7                \n",
      "  hive-jdbc.noarch 0:1.1.0+cdh5.9.0+752-1.cdh5.9.0.p0.30.el7                    \n",
      "  sentry.noarch 0:1.5.1+cdh5.9.0+261-1.cdh5.9.0.p0.30.el7                       \n",
      "  solr.noarch 0:4.10.3+cdh5.9.0+458-1.cdh5.9.0.p0.30.el7                        \n",
      "\n",
      "Complete!\n"
     ]
    }
   ],
   "source": [
    "! yum install -y dos2unix pig hbase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copiamos los ficheros de datos al directorio de trabajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 38M\r\n",
      "-rw-r--r-- 1 root root 1.8K Feb 18 10:03 forum1.tsv\r\n",
      "-rwxr-xr-x 1 root root  38M Feb 18 10:03 forum_node.tsv.gz\r\n"
     ]
    }
   ],
   "source": [
    "! cp ../dataset/forum_node.tsv.gz ../dataset/forum1.tsv .\n",
    "! ls -lh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descomprimimos el primer fichero y lo limpiamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dos2unix: converting file forum_node.tsv to Unix format ...\r\n"
     ]
    }
   ],
   "source": [
    "! gzip -d forum_node.tsv.gz && dos2unix -f forum_node.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el directorio de usuario en Hadoop si no existiera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/root/pig-indiceinvertido': No such file or directory\n",
      "rm: `/user/root/inverted_index': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -rm -r /user/$(whoami)/pig-indiceinvertido\n",
    "! hadoop fs -rm -r /user/$(whoami)/inverted_index\n",
    "! hadoop fs -mkdir -p /user/$(whoami)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copiamos los ficheros a Hadoop y al directorio local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "! hadoop fs -put -p forum_node.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "! hadoop fs -put forum1.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   3 root supergroup       1774 2018-02-18 10:03 forum1.tsv\r\n",
      "-rwxr-xr-x   3 root root        120109135 2018-02-18 10:03 forum_node.tsv\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creación del fichero PIG a ejecutar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing students-inverted-index.pig\n"
     ]
    }
   ],
   "source": [
    "%%writefile students-inverted-index.pig\n",
    "\n",
    "/* 1.Carga el fichero de los posts forum_node.tsv, utilizando una extension de Piggybank para poder quitar la cabecera,\n",
    "en vez de usar directamente el PigStorage. */\n",
    "REGISTER /usr/lib/pig/piggybank.jar;\n",
    "DEFINE StringToInt InvokeForInt('java.lang.Integer.valueOf', 'String');\n",
    "\n",
    "data =\n",
    "    load 'forum_node.tsv'\n",
    "    using org.apache.pig.piggybank.storage.CSVExcelStorage('\\t', 'YES_MULTILINE', 'NOCHANGE', 'SKIP_INPUT_HEADER')\n",
    "    as (pid:chararray, title:chararray, tagnames:chararray,\n",
    "        author_id:chararray,body:chararray,\n",
    "        node_type:chararray, parent_id:chararray,\n",
    "        abs_parent_id:chararray,added_at:chararray,\n",
    "        score:chararray, state_string:chararray, last_edited_id:chararray,\n",
    "        last_activity_by_id:chararray, last_activity_at:chararray,\n",
    "        active_revision_id:chararray, extra:chararray,\n",
    "        extra_ref_id:chararray, extra_count:chararray, marked:chararray);\n",
    "\n",
    "/* 2.Limpiamos el fichero quitando los saltos de linea, expresiones html y la expresión regular que se proponia en el ejercicio. */\n",
    "cleandata = foreach data generate\n",
    "    REPLACE(pid, '[a-zA-Z]+', '') as post_id,\n",
    "    LOWER(REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(body, '\\\\\\\\n\\\\\\\\r', ''), '\\\\\\\\r', ''), '\\\\\\\\n', ''), '<*>', ''), '[^a-zA-Z0-9\\'\\\\s]+', ' ')) AS clean_body;\n",
    "\n",
    "/* 3.Filtramos los datos de post_id que no son numericos. */\n",
    "cleandata_filtered = filter cleandata by org.apache.pig.piggybank.evaluation.IsNumeric(post_id);\n",
    "\n",
    "/* 4.Creamos tuplas separando el body por espacios y convirtiendo el post_id en un numerico a través de una función custom, para evitar problemas que sufrimos con el cast de String a Integer. */\n",
    "words_data = FOREACH cleandata_filtered GENERATE StringToInt(post_id) as post_id_int:int, FLATTEN(TOKENIZE(clean_body)) as word;\n",
    "words_data_filtered = filter words_data by SIZE(word) > 0;\n",
    "\n",
    "/* 5.Agrupamos por palabra */\n",
    "word_groups = GROUP words_data_filtered BY word;\n",
    "\n",
    "/* 6.Por cada grupo de palabras, hacemos un distinct para los post_id, eliminando los duplicados, contamos el número de post en que aparece (despues de quitar los duplicados) y generamos una fila con el índice. */\n",
    "index = FOREACH word_groups {\n",
    "    pairs = DISTINCT $1.$0;\n",
    "    cnt = COUNT(pairs);\n",
    "    GENERATE $0 as word, pairs as index_bag, cnt as count;\n",
    "};\n",
    "\n",
    "/* 7.Como se pide que el indice lleve el post_id ordenador, ordenamos la bag resultante de los posts por su id. */\n",
    "sorted_index = foreach index {\n",
    "    sorted_bag = order index_bag by $0;\n",
    "    generate word, sorted_bag, count;\n",
    "}\n",
    "\n",
    "/* 8. Lo guardamos en un fichero. */\n",
    "STORE sorted_index INTO 'inverted_index';\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ejecución del fichero PIG en modo local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log4j:WARN No appenders could be found for logger (org.apache.hadoop.util.Shell).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n",
      "2018-02-18 10:04:18,343 [main] INFO  org.apache.pig.Main - Apache Pig version 0.12.0-cdh5.9.0 (rUnversioned directory) compiled Oct 21 2016, 01:17:18\n",
      "2018-02-18 10:04:18,344 [main] INFO  org.apache.pig.Main - Logging error messages to: /media/notebooks/pig-indiceinvertido/pig_1518948258309.log\n",
      "2018-02-18 10:04:18,369 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "2018-02-18 10:04:18,781 [main] INFO  org.apache.pig.impl.util.Utils - Default bootup file /root/.pigbootup not found\n",
      "2018-02-18 10:04:18,887 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-18 10:04:18,888 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "2018-02-18 10:04:18,889 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: file:///\n",
      "2018-02-18 10:04:18,961 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-18 10:04:19,045 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-18 10:04:19,141 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-18 10:04:19,208 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-18 10:04:19,292 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-18 10:04:19,392 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-18 10:04:19,453 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-18 10:04:19,527 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-18 10:04:19,657 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-18 10:04:20,743 [main] WARN  org.apache.pig.PigServer - Encountered Warning IMPLICIT_CAST_TO_LONG 1 time(s).\n",
      "2018-02-18 10:04:20,775 [main] INFO  org.apache.pig.tools.pigstats.ScriptState - Pig features used in the script: GROUP_BY,FILTER\n",
      "2018-02-18 10:04:20,889 [main] INFO  org.apache.pig.newplan.logical.optimizer.LogicalPlanOptimizer - {RULES_ENABLED=[AddForEach, ColumnMapKeyPrune, DuplicateForEachColumnRewrite, GroupByConstParallelSetter, ImplicitSplitInserter, LimitOptimizer, LoadTypeCastInserter, MergeFilter, MergeForEach, NewPartitionFilterOptimizer, PushDownForEachFlatten, PushUpFilter, SplitFilter, StreamTypeCastInserter], RULES_DISABLED=[FilterLogicExpressionSimplifier, PartitionFilterOptimizer]}\n",
      "2018-02-18 10:04:20,945 [main] INFO  org.apache.pig.newplan.logical.rules.ColumnPruneVisitor - Columns pruned for data: $1, $2, $3, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15, $16, $17, $18\n",
      "2018-02-18 10:04:20,972 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.textoutputformat.separator is deprecated. Instead, use mapreduce.output.textoutputformat.separator\n",
      "2018-02-18 10:04:21,180 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler - File concatenation threshold: 100 optimistic? false\n",
      "2018-02-18 10:04:21,250 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size before optimization: 1\n",
      "2018-02-18 10:04:21,250 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size after optimization: 1\n",
      "2018-02-18 10:04:21,311 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "2018-02-18 10:04:21,314 [main] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "2018-02-18 10:04:21,366 [main] INFO  org.apache.pig.tools.pigstats.ScriptState - Pig script settings are added to the job\n",
      "2018-02-18 10:04:21,509 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.reduce.markreset.buffer.percent is deprecated. Instead, use mapreduce.reduce.markreset.buffer.percent\n",
      "2018-02-18 10:04:21,510 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3\n",
      "2018-02-18 10:04:21,510 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress\n",
      "2018-02-18 10:04:21,516 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Reduce phase detected, estimating # of required reducers.\n",
      "2018-02-18 10:04:21,518 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Using reducer estimator: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.InputSizeReducerEstimator\n",
      "2018-02-18 10:04:21,525 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.InputSizeReducerEstimator - BytesPerReducer=1000000000 maxReducers=999 totalInputFileSize=120109135\n",
      "2018-02-18 10:04:21,525 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting Parallelism to 1\n",
      "2018-02-18 10:04:21,525 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "2018-02-18 10:04:21,574 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting up single store job\n",
      "2018-02-18 10:04:21,594 [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Key [pig.schematuple] is false, will not generate code.\n",
      "2018-02-18 10:04:21,594 [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Starting process to move generated code to distributed cache\n",
      "2018-02-18 10:04:21,594 [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Distributed cache not supported or needed in local mode. Setting key [pig.schematuple.local.dir] with code temp directory: /tmp/1518948261593-0\n",
      "2018-02-18 10:04:21,810 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 1 map-reduce job(s) waiting for submission.\n",
      "2018-02-18 10:04:21,811 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.tracker.http.address is deprecated. Instead, use mapreduce.jobtracker.http.address\n",
      "2018-02-18 10:04:21,841 [JobControl] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "2018-02-18 10:04:22,182 [JobControl] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n",
      "2018-02-18 10:04:22,251 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths to process : 1\n",
      "2018-02-18 10:04:22,253 [JobControl] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths to process : 1\n",
      "2018-02-18 10:04:22,315 [JobControl] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths (combined) to process : 4\n",
      "2018-02-18 10:04:22,384 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:4\n",
      "2018-02-18 10:04:22,403 [JobControl] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-18 10:04:22,541 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_local811689810_0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-18 10:04:22,801 [JobControl] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://localhost:8080/\n",
      "2018-02-18 10:04:22,803 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - HadoopJobId: job_local811689810_0001\n",
      "2018-02-18 10:04:22,803 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Processing aliases 1-1,cleandata,cleandata_filtered,data,index,pairs,sorted_bag,sorted_index,word_groups,words_data,words_data_filtered\n",
      "2018-02-18 10:04:22,803 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - detailed locations: M: data[8,4],cleandata[-1,-1],cleandata_filtered[25,21],words_data[28,13],words_data_filtered[29,22],word_groups[32,14] C:  R: index[35,8],1-1[36,21],pairs[36,12],1-1[36,21],pairs[36,12],sorted_index[42,15],sorted_bag[43,17]\n",
      "2018-02-18 10:04:22,808 [Thread-5] INFO  org.apache.hadoop.mapred.LocalJobRunner - OutputCommitter set in config null\n",
      "2018-02-18 10:04:22,813 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 0% complete\n",
      "2018-02-18 10:04:22,835 [Thread-5] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.textoutputformat.separator is deprecated. Instead, use mapreduce.output.textoutputformat.separator\n",
      "2018-02-18 10:04:22,836 [Thread-5] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "2018-02-18 10:04:22,836 [Thread-5] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.reduce.markreset.buffer.percent is deprecated. Instead, use mapreduce.reduce.markreset.buffer.percent\n",
      "2018-02-18 10:04:22,837 [Thread-5] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "2018-02-18 10:04:22,837 [Thread-5] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-18 10:04:22,838 [Thread-5] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "2018-02-18 10:04:22,840 [Thread-5] INFO  org.apache.hadoop.mapred.LocalJobRunner - OutputCommitter is org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter\n",
      "2018-02-18 10:04:22,942 [Thread-5] INFO  org.apache.hadoop.mapred.LocalJobRunner - Waiting for map tasks\n",
      "2018-02-18 10:04:22,943 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Starting task: attempt_local811689810_0001_m_000000_0\n",
      "2018-02-18 10:04:23,061 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "2018-02-18 10:04:23,100 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task -  Using ResourceCalculatorProcessTree : [ ]\n",
      "2018-02-18 10:04:23,116 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Processing split: Number of splits :1\n",
      "Total Length = 33554432\n",
      "Input split[0]:\n",
      "   Length = 33554432\n",
      "  Locations:\n",
      "\n",
      "-----------------------\n",
      "\n",
      "2018-02-18 10:04:23,149 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader - Current split being processed file:/media/notebooks/pig-indiceinvertido/forum_node.tsv:0+33554432\n",
      "2018-02-18 10:04:23,214 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - (EQUATOR) 0 kvi 26214396(104857584)\n",
      "2018-02-18 10:04:23,215 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - mapreduce.task.io.sort.mb: 100\n",
      "2018-02-18 10:04:23,215 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - soft limit at 83886080\n",
      "2018-02-18 10:04:23,215 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 0; bufvoid = 104857600\n",
      "2018-02-18 10:04:23,215 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 26214396; length = 6553600\n",
      "2018-02-18 10:04:23,225 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "2018-02-18 10:04:23,410 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.data.SchemaTupleBackend - Key [pig.schematuple] was not set... will not generate code.\n",
      "2018-02-18 10:04:23,452 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Map - Aliases being processed per job phase (AliasName[line,offset]): M: data[8,4],cleandata[-1,-1],cleandata_filtered[25,21],words_data[28,13],words_data_filtered[29,22],word_groups[32,14] C:  R: index[35,8],1-1[36,21],pairs[36,12],1-1[36,21],pairs[36,12],sorted_index[42,15],sorted_bag[43,17]\n",
      "2018-02-18 10:04:29,303 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map\n",
      "2018-02-18 10:04:32,305 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map\n",
      "2018-02-18 10:04:32,366 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Spilling map output\n",
      "2018-02-18 10:04:32,366 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 0; bufend = 39858232; bufvoid = 104857600\n",
      "2018-02-18 10:04:32,366 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 26214396(104857584); kvend = 15207436(60829744); length = 11006961/6553600\n",
      "2018-02-18 10:04:32,366 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - (EQUATOR) 50343980 kvi 12585988(50343952)\n",
      "2018-02-18 10:04:35,306 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map\n",
      "2018-02-18 10:04:37,125 [SpillThread] INFO  org.apache.hadoop.mapred.MapTask - Finished spill 0\n",
      "2018-02-18 10:04:37,127 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - (RESET) equator 50343980 kv 12585988(50343952) kvi 9964564(39858256)\n",
      "2018-02-18 10:04:37,864 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map\n",
      "2018-02-18 10:04:37,867 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Starting flush of map output\n",
      "2018-02-18 10:04:37,867 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Spilling map output\n",
      "2018-02-18 10:04:37,867 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 50343980; bufend = 63284165; bufvoid = 104857600\n",
      "2018-02-18 10:04:37,867 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 12585988(50343952); kvend = 9125100(36500400); length = 3460889/6553600\n",
      "2018-02-18 10:04:38,312 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > sort\n",
      "2018-02-18 10:04:39,049 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Finished spill 1\n",
      "2018-02-18 10:04:39,075 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Merger - Merging 2 sorted segments\n",
      "2018-02-18 10:04:39,108 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Merger - Down to the last merge-pass, with 2 segments left of total size: 60032390 bytes\n",
      "2018-02-18 10:04:41,314 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > sort > \n",
      "2018-02-18 10:04:41,369 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Task:attempt_local811689810_0001_m_000000_0 is done. And is in the process of committing\n",
      "2018-02-18 10:04:41,385 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > sort\n",
      "2018-02-18 10:04:41,386 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Task 'attempt_local811689810_0001_m_000000_0' done.\n",
      "2018-02-18 10:04:41,387 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Finishing task: attempt_local811689810_0001_m_000000_0\n",
      "2018-02-18 10:04:41,388 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Starting task: attempt_local811689810_0001_m_000001_0\n",
      "2018-02-18 10:04:41,404 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "2018-02-18 10:04:41,408 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task -  Using ResourceCalculatorProcessTree : [ ]\n",
      "2018-02-18 10:04:41,414 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Processing split: Number of splits :1\n",
      "Total Length = 33554432\n",
      "Input split[0]:\n",
      "   Length = 33554432\n",
      "  Locations:\n",
      "\n",
      "-----------------------\n",
      "\n",
      "2018-02-18 10:04:41,420 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader - Current split being processed file:/media/notebooks/pig-indiceinvertido/forum_node.tsv:33554432+33554432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-18 10:04:41,438 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - (EQUATOR) 0 kvi 26214396(104857584)\n",
      "2018-02-18 10:04:41,439 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - mapreduce.task.io.sort.mb: 100\n",
      "2018-02-18 10:04:41,439 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - soft limit at 83886080\n",
      "2018-02-18 10:04:41,439 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 0; bufvoid = 104857600\n",
      "2018-02-18 10:04:41,439 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 26214396; length = 6553600\n",
      "2018-02-18 10:04:41,440 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "2018-02-18 10:04:41,470 [LocalJobRunner Map Task Executor #0] WARN  org.apache.pig.data.SchemaTupleBackend - SchemaTupleBackend has already been initialized\n",
      "2018-02-18 10:04:41,501 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Map - Aliases being processed per job phase (AliasName[line,offset]): M: data[8,4],cleandata[-1,-1],cleandata_filtered[25,21],words_data[28,13],words_data_filtered[29,22],word_groups[32,14] C:  R: index[35,8],1-1[36,21],pairs[36,12],1-1[36,21],pairs[36,12],sorted_index[42,15],sorted_bag[43,17]\n",
      "2018-02-18 10:04:47,401 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map\n",
      "2018-02-18 10:04:49,746 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Spilling map output\n",
      "2018-02-18 10:04:49,747 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 0; bufend = 40782076; bufvoid = 104857600\n",
      "2018-02-18 10:04:49,747 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 26214396(104857584); kvend = 15438396(61753584); length = 10776001/6553600\n",
      "2018-02-18 10:04:49,747 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - (EQUATOR) 51267822 kvi 12816948(51267792)\n",
      "2018-02-18 10:04:50,402 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map\n",
      "2018-02-18 10:04:53,403 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map\n",
      "2018-02-18 10:04:53,961 [SpillThread] INFO  org.apache.hadoop.mapred.MapTask - Finished spill 0\n",
      "2018-02-18 10:04:53,961 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - (RESET) equator 51267822 kv 12816948(51267792) kvi 10195524(40782096)\n",
      "2018-02-18 10:04:55,354 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map\n",
      "2018-02-18 10:04:55,354 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Starting flush of map output\n",
      "2018-02-18 10:04:55,354 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Spilling map output\n",
      "2018-02-18 10:04:55,354 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 51267822; bufend = 69773215; bufvoid = 104857600\n",
      "2018-02-18 10:04:55,354 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 12816948(51267792); kvend = 7966940(31867760); length = 4850009/6553600\n",
      "2018-02-18 10:04:56,404 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > sort\n",
      "2018-02-18 10:04:56,973 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Finished spill 1\n",
      "2018-02-18 10:04:56,981 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Merger - Merging 2 sorted segments\n",
      "2018-02-18 10:04:56,987 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Merger - Down to the last merge-pass, with 2 segments left of total size: 67100520 bytes\n",
      "2018-02-18 10:04:59,035 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Task:attempt_local811689810_0001_m_000001_0 is done. And is in the process of committing\n",
      "2018-02-18 10:04:59,037 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > sort\n",
      "2018-02-18 10:04:59,037 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Task 'attempt_local811689810_0001_m_000001_0' done.\n",
      "2018-02-18 10:04:59,037 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Finishing task: attempt_local811689810_0001_m_000001_0\n",
      "2018-02-18 10:04:59,037 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Starting task: attempt_local811689810_0001_m_000002_0\n",
      "2018-02-18 10:04:59,043 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "2018-02-18 10:04:59,043 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task -  Using ResourceCalculatorProcessTree : [ ]\n",
      "2018-02-18 10:04:59,045 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Processing split: Number of splits :1\n",
      "Total Length = 33554432\n",
      "Input split[0]:\n",
      "   Length = 33554432\n",
      "  Locations:\n",
      "\n",
      "-----------------------\n",
      "\n",
      "2018-02-18 10:04:59,048 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader - Current split being processed file:/media/notebooks/pig-indiceinvertido/forum_node.tsv:67108864+33554432\n",
      "2018-02-18 10:04:59,062 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - (EQUATOR) 0 kvi 26214396(104857584)\n",
      "2018-02-18 10:04:59,062 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - mapreduce.task.io.sort.mb: 100\n",
      "2018-02-18 10:04:59,062 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - soft limit at 83886080\n",
      "2018-02-18 10:04:59,062 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 0; bufvoid = 104857600\n",
      "2018-02-18 10:04:59,062 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 26214396; length = 6553600\n",
      "2018-02-18 10:04:59,074 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "2018-02-18 10:04:59,098 [LocalJobRunner Map Task Executor #0] WARN  org.apache.pig.data.SchemaTupleBackend - SchemaTupleBackend has already been initialized\n",
      "2018-02-18 10:04:59,107 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Map - Aliases being processed per job phase (AliasName[line,offset]): M: data[8,4],cleandata[-1,-1],cleandata_filtered[25,21],words_data[28,13],words_data_filtered[29,22],word_groups[32,14] C:  R: index[35,8],1-1[36,21],pairs[36,12],1-1[36,21],pairs[36,12],sorted_index[42,15],sorted_bag[43,17]\n",
      "2018-02-18 10:05:05,047 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map\n",
      "2018-02-18 10:05:06,280 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Spilling map output\n",
      "2018-02-18 10:05:06,280 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 0; bufend = 40677656; bufvoid = 104857600\n",
      "2018-02-18 10:05:06,280 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 26214396(104857584); kvend = 15412296(61649184); length = 10802101/6553600\n",
      "2018-02-18 10:05:06,280 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - (EQUATOR) 51163412 kvi 12790848(51163392)\n",
      "2018-02-18 10:05:08,049 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map\n",
      "2018-02-18 10:05:09,983 [SpillThread] INFO  org.apache.hadoop.mapred.MapTask - Finished spill 0\n",
      "2018-02-18 10:05:09,983 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - (RESET) equator 51163412 kv 12790848(51163392) kvi 10169420(40677680)\n",
      "2018-02-18 10:05:11,051 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map\n",
      "2018-02-18 10:05:11,565 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map\n",
      "2018-02-18 10:05:11,565 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Starting flush of map output\n",
      "2018-02-18 10:05:11,565 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Spilling map output\n",
      "2018-02-18 10:05:11,565 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 51163412; bufend = 70737927; bufvoid = 104857600\n",
      "2018-02-18 10:05:11,565 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 12790848(51163392); kvend = 7585048(30340192); length = 5205801/6553600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-18 10:05:13,212 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Finished spill 1\n",
      "2018-02-18 10:05:13,214 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Merger - Merging 2 sorted segments\n",
      "2018-02-18 10:05:13,216 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Merger - Down to the last merge-pass, with 2 segments left of total size: 68256169 bytes\n",
      "2018-02-18 10:05:14,057 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > sort > \n",
      "2018-02-18 10:05:15,210 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Task:attempt_local811689810_0001_m_000002_0 is done. And is in the process of committing\n",
      "2018-02-18 10:05:15,212 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > sort\n",
      "2018-02-18 10:05:15,213 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Task 'attempt_local811689810_0001_m_000002_0' done.\n",
      "2018-02-18 10:05:15,213 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Finishing task: attempt_local811689810_0001_m_000002_0\n",
      "2018-02-18 10:05:15,213 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Starting task: attempt_local811689810_0001_m_000003_0\n",
      "2018-02-18 10:05:15,220 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "2018-02-18 10:05:15,221 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task -  Using ResourceCalculatorProcessTree : [ ]\n",
      "2018-02-18 10:05:15,227 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Processing split: Number of splits :1\n",
      "Total Length = 19445839\n",
      "Input split[0]:\n",
      "   Length = 19445839\n",
      "  Locations:\n",
      "\n",
      "-----------------------\n",
      "\n",
      "2018-02-18 10:05:15,229 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader - Current split being processed file:/media/notebooks/pig-indiceinvertido/forum_node.tsv:100663296+19445839\n",
      "2018-02-18 10:05:15,243 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - (EQUATOR) 0 kvi 26214396(104857584)\n",
      "2018-02-18 10:05:15,243 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - mapreduce.task.io.sort.mb: 100\n",
      "2018-02-18 10:05:15,243 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - soft limit at 83886080\n",
      "2018-02-18 10:05:15,243 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 0; bufvoid = 104857600\n",
      "2018-02-18 10:05:15,243 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 26214396; length = 6553600\n",
      "2018-02-18 10:05:15,244 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "2018-02-18 10:05:15,271 [LocalJobRunner Map Task Executor #0] WARN  org.apache.pig.data.SchemaTupleBackend - SchemaTupleBackend has already been initialized\n",
      "2018-02-18 10:05:15,287 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Map - Aliases being processed per job phase (AliasName[line,offset]): M: data[8,4],cleandata[-1,-1],cleandata_filtered[25,21],words_data[28,13],words_data_filtered[29,22],word_groups[32,14] C:  R: index[35,8],1-1[36,21],pairs[36,12],1-1[36,21],pairs[36,12],sorted_index[42,15],sorted_bag[43,17]\n",
      "2018-02-18 10:05:21,219 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map\n",
      "2018-02-18 10:05:22,416 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map\n",
      "2018-02-18 10:05:22,416 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Starting flush of map output\n",
      "2018-02-18 10:05:22,416 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Spilling map output\n",
      "2018-02-18 10:05:22,416 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 0; bufend = 40451886; bufvoid = 104857600\n",
      "2018-02-18 10:05:22,416 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 26214396(104857584); kvend = 15404404(61617616); length = 10809993/6553600\n",
      "2018-02-18 10:05:24,222 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > sort\n",
      "2018-02-18 10:05:25,860 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Finished spill 0\n",
      "2018-02-18 10:05:25,861 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Task:attempt_local811689810_0001_m_000003_0 is done. And is in the process of committing\n",
      "2018-02-18 10:05:25,863 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - map\n",
      "2018-02-18 10:05:25,863 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Task 'attempt_local811689810_0001_m_000003_0' done.\n",
      "2018-02-18 10:05:25,863 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Finishing task: attempt_local811689810_0001_m_000003_0\n",
      "2018-02-18 10:05:25,863 [Thread-5] INFO  org.apache.hadoop.mapred.LocalJobRunner - map task executor complete.\n",
      "2018-02-18 10:05:25,877 [Thread-5] INFO  org.apache.hadoop.mapred.LocalJobRunner - Waiting for reduce tasks\n",
      "2018-02-18 10:05:25,877 [pool-5-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - Starting task: attempt_local811689810_0001_r_000000_0\n",
      "2018-02-18 10:05:25,911 [pool-5-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "2018-02-18 10:05:25,913 [pool-5-thread-1] INFO  org.apache.hadoop.mapred.Task -  Using ResourceCalculatorProcessTree : [ ]\n",
      "2018-02-18 10:05:25,923 [pool-5-thread-1] INFO  org.apache.hadoop.mapred.ReduceTask - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@556011b7\n",
      "2018-02-18 10:05:25,962 [pool-5-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - MergerManager: memoryLimit=652528832, maxSingleShuffleLimit=163132208, mergeThreshold=430669056, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "2018-02-18 10:05:25,967 [EventFetcher for fetching Map Completion Events] INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher - attempt_local811689810_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "2018-02-18 10:05:26,103 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.LocalFetcher - localfetcher#1 about to shuffle output of map attempt_local811689810_0001_m_000002_0 decomp: 68256171 len: 68256175 to MEMORY\n",
      "2018-02-18 10:05:26,223 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput - Read 68256171 bytes from map-output for attempt_local811689810_0001_m_000002_0\n",
      "2018-02-18 10:05:26,228 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - closeInMemoryFile -> map-output of size: 68256171, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->68256171\n",
      "2018-02-18 10:05:26,242 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.LocalFetcher - localfetcher#1 about to shuffle output of map attempt_local811689810_0001_m_000001_0 decomp: 67100522 len: 67100526 to MEMORY\n",
      "2018-02-18 10:05:26,515 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput - Read 67100522 bytes from map-output for attempt_local811689810_0001_m_000001_0\n",
      "2018-02-18 10:05:26,516 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - closeInMemoryFile -> map-output of size: 67100522, inMemoryMapOutputs.size() -> 2, commitMemory -> 68256171, usedMemory ->135356693\n",
      "2018-02-18 10:05:26,523 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.LocalFetcher - localfetcher#1 about to shuffle output of map attempt_local811689810_0001_m_000003_0 decomp: 45856899 len: 45856903 to MEMORY\n",
      "2018-02-18 10:05:26,574 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput - Read 45856899 bytes from map-output for attempt_local811689810_0001_m_000003_0\n",
      "2018-02-18 10:05:26,574 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - closeInMemoryFile -> map-output of size: 45856899, inMemoryMapOutputs.size() -> 3, commitMemory -> 135356693, usedMemory ->181213592\n",
      "2018-02-18 10:05:26,591 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.LocalFetcher - localfetcher#1 about to shuffle output of map attempt_local811689810_0001_m_000000_0 decomp: 60032392 len: 60032396 to MEMORY\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-18 10:05:26,841 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput - Read 60032392 bytes from map-output for attempt_local811689810_0001_m_000000_0\n",
      "2018-02-18 10:05:26,842 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - closeInMemoryFile -> map-output of size: 60032392, inMemoryMapOutputs.size() -> 4, commitMemory -> 181213592, usedMemory ->241245984\n",
      "2018-02-18 10:05:26,843 [EventFetcher for fetching Map Completion Events] INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher - EventFetcher is interrupted.. Returning\n",
      "2018-02-18 10:05:26,844 [pool-5-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - 4 / 4 copied.\n",
      "2018-02-18 10:05:26,846 [pool-5-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - finalMerge called with 4 in-memory map-outputs and 0 on-disk map-outputs\n",
      "2018-02-18 10:05:26,981 [pool-5-thread-1] INFO  org.apache.hadoop.mapred.Merger - Merging 4 sorted segments\n",
      "2018-02-18 10:05:26,981 [pool-5-thread-1] INFO  org.apache.hadoop.mapred.Merger - Down to the last merge-pass, with 4 segments left of total size: 241245960 bytes\n",
      "2018-02-18 10:05:27,035 [Service Thread] INFO  org.apache.pig.impl.util.SpillableMemoryManager - first memory handler call - Collection threshold init = 43515904(42496K) used = 357110976(348741K) committed = 676855808(660992K) max = 699400192(683008K)\n",
      "2018-02-18 10:05:31,877 [pool-5-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - Merged 4 segments, 241245984 bytes to disk to satisfy reduce memory limit\n",
      "2018-02-18 10:05:31,878 [pool-5-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - Merging 1 files, 241245982 bytes from disk\n",
      "2018-02-18 10:05:31,884 [pool-5-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - Merging 0 segments, 0 bytes from memory into reduce\n",
      "2018-02-18 10:05:31,884 [pool-5-thread-1] INFO  org.apache.hadoop.mapred.Merger - Merging 1 sorted segments\n",
      "2018-02-18 10:05:31,885 [pool-5-thread-1] INFO  org.apache.hadoop.mapred.Merger - Down to the last merge-pass, with 1 segments left of total size: 241245972 bytes\n",
      "2018-02-18 10:05:31,885 [pool-5-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - 4 / 4 copied.\n",
      "2018-02-18 10:05:31,898 [pool-5-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "2018-02-18 10:05:31,912 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-18 10:05:31,933 [pool-5-thread-1] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "2018-02-18 10:05:31,954 [pool-5-thread-1] WARN  org.apache.pig.data.SchemaTupleBackend - SchemaTupleBackend has already been initialized\n",
      "2018-02-18 10:05:31,971 [pool-5-thread-1] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce - Aliases being processed per job phase (AliasName[line,offset]): M: data[8,4],cleandata[-1,-1],cleandata_filtered[25,21],words_data[28,13],words_data_filtered[29,22],word_groups[32,14] C:  R: index[35,8],1-1[36,21],pairs[36,12],1-1[36,21],pairs[36,12],sorted_index[42,15],sorted_bag[43,17]\n",
      "2018-02-18 10:05:34,913 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-18 10:05:37,915 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-18 10:05:40,915 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-18 10:05:43,916 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-18 10:05:46,151 [Service Thread] INFO  org.apache.pig.impl.util.SpillableMemoryManager - first memory handler call- Usage threshold init = 43515904(42496K) used = 510205024(498247K) committed = 676855808(660992K) max = 699400192(683008K)\n",
      "2018-02-18 10:05:46,678 [Service Thread] INFO  org.apache.pig.impl.util.SpillableMemoryManager - Spilled an estimate of 148664201 bytes from 12 objects. init = 43515904(42496K) used = 510205024(498247K) committed = 676855808(660992K) max = 699400192(683008K)\n",
      "2018-02-18 10:05:46,918 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-18 10:05:49,919 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-18 10:05:52,920 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-18 10:05:55,922 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-18 10:05:58,923 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-18 10:06:01,931 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-18 10:06:04,935 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-18 10:06:07,944 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-18 10:06:10,945 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-18 10:06:13,946 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-18 10:06:16,948 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-18 10:06:19,950 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-18 10:06:22,951 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-18 10:06:25,953 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-18 10:06:28,960 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-18 10:06:31,961 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-18 10:06:34,963 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-18 10:06:37,966 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-18 10:06:40,967 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-18 10:06:43,970 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-18 10:06:46,971 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-18 10:06:49,973 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-18 10:06:52,975 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-18 10:06:55,979 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-18 10:06:57,852 [pool-5-thread-1] INFO  org.apache.hadoop.mapred.Task - Task:attempt_local811689810_0001_r_000000_0 is done. And is in the process of committing\n",
      "2018-02-18 10:06:57,861 [pool-5-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-18 10:06:57,861 [pool-5-thread-1] INFO  org.apache.hadoop.mapred.Task - Task attempt_local811689810_0001_r_000000_0 is allowed to commit now\n",
      "2018-02-18 10:06:57,871 [pool-5-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_local811689810_0001_r_000000_0' to file:/media/notebooks/pig-indiceinvertido/inverted_index/_temporary/0/task_local811689810_0001_r_000000\n",
      "2018-02-18 10:06:57,873 [pool-5-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-18 10:06:57,873 [pool-5-thread-1] INFO  org.apache.hadoop.mapred.Task - Task 'attempt_local811689810_0001_r_000000_0' done.\n",
      "2018-02-18 10:06:57,873 [pool-5-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - Finishing task: attempt_local811689810_0001_r_000000_0\n",
      "2018-02-18 10:06:57,874 [Thread-5] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce task executor complete.\n",
      "2018-02-18 10:06:58,406 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "2018-02-18 10:06:58,406 [main] WARN  org.apache.pig.tools.pigstats.PigStatsUtil - Failed to get RunningJob for job job_local811689810_0001\n",
      "2018-02-18 10:06:58,417 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 100% complete\n",
      "2018-02-18 10:06:58,417 [main] INFO  org.apache.pig.tools.pigstats.SimplePigStats - Detected Local mode. Stats reported below may be incomplete\n",
      "2018-02-18 10:06:58,428 [main] INFO  org.apache.pig.tools.pigstats.SimplePigStats - Script Statistics: \n",
      "\n",
      "HadoopVersion\tPigVersion\tUserId\tStartedAt\tFinishedAt\tFeatures\n",
      "2.6.0-cdh5.9.0\t0.12.0-cdh5.9.0\troot\t2018-02-18 10:04:21\t2018-02-18 10:06:58\tGROUP_BY,FILTER\n",
      "\n",
      "Success!\n",
      "\n",
      "Job Stats (time in seconds):\n",
      "JobId\tAlias\tFeature\tOutputs\n",
      "job_local811689810_0001\t1-1,cleandata,cleandata_filtered,data,index,pairs,sorted_bag,sorted_index,word_groups,words_data,words_data_filtered\tGROUP_BY,DISTINCT\tfile:///media/notebooks/pig-indiceinvertido/inverted_index,\n",
      "\n",
      "Input(s):\n",
      "Successfully read records from: \"file:///media/notebooks/pig-indiceinvertido/forum_node.tsv\"\n",
      "\n",
      "Output(s):\n",
      "Successfully stored records in: \"file:///media/notebooks/pig-indiceinvertido/inverted_index\"\n",
      "\n",
      "Job DAG:\n",
      "job_local811689810_0001\n",
      "\n",
      "\n",
      "2018-02-18 10:06:58,429 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Success!\n"
     ]
    }
   ],
   "source": [
    "! pig -f students-inverted-index.pig -x local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "googlecode\t{(79),(10617),(28844),(45630),(1006011),(1034790),(3001942),(5001673),(5002485),(6001946),(6002641),(6002651),(6002660),(6002690),(6003146),(6003978),(6004231),(6004298),(6005746),(6006014),(6007830),(6010307),(6015242),(6016801),(6022761),(6025168),(6027118),(7006210)}\t28\r\n",
      "googlemail\t{(66818),(66848),(67057),(6005844),(6006822),(6008744)}\t6\r\n",
      "googlemale\t{(10011506)}\t1\r\n",
      "googleplex\t{(2009842)}\t1\r\n",
      "googleplus\t{(6002796)}\t1\r\n",
      "googolplex\t{(2009965)}\t1\r\n",
      "goosebumps\t{(5002139)}\t1\r\n",
      "gorchynski\t{(12002258),(12002259),(12002270),(12002296),(12002300),(12002345),(12002440)}\t7\r\n",
      "gordeychuk\t{(12003403)}\t1\r\n",
      "gorilla834\t{(6019443),(6019538)}\t2\r\n",
      "tail: write error: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "! tail -60000 ./inverted_index/part-r-00000  | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ejecución del fichero PIG en Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log4j:WARN No appenders could be found for logger (org.apache.hadoop.util.Shell).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n",
      "2018-02-18 10:07:32,413 [main] INFO  org.apache.pig.Main - Apache Pig version 0.12.0-cdh5.9.0 (rUnversioned directory) compiled Oct 21 2016, 01:17:18\n",
      "2018-02-18 10:07:32,416 [main] INFO  org.apache.pig.Main - Logging error messages to: /media/notebooks/pig-indiceinvertido/pig_1518948452352.log\n",
      "2018-02-18 10:07:34,140 [main] INFO  org.apache.pig.impl.util.Utils - Default bootup file /root/.pigbootup not found\n",
      "2018-02-18 10:07:34,408 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "2018-02-18 10:07:34,408 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-18 10:07:34,408 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: hdfs://namenode:8020\n",
      "2018-02-18 10:07:35,625 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-18 10:07:35,698 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-18 10:07:35,771 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-18 10:07:35,874 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-18 10:07:35,955 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-18 10:07:36,053 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-18 10:07:36,135 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-18 10:07:36,235 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-18 10:07:36,352 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-18 10:07:37,920 [main] WARN  org.apache.pig.PigServer - Encountered Warning IMPLICIT_CAST_TO_LONG 1 time(s).\n",
      "2018-02-18 10:07:37,941 [main] INFO  org.apache.pig.tools.pigstats.ScriptState - Pig features used in the script: GROUP_BY,FILTER\n",
      "2018-02-18 10:07:38,031 [main] INFO  org.apache.pig.newplan.logical.optimizer.LogicalPlanOptimizer - {RULES_ENABLED=[AddForEach, ColumnMapKeyPrune, DuplicateForEachColumnRewrite, GroupByConstParallelSetter, ImplicitSplitInserter, LimitOptimizer, LoadTypeCastInserter, MergeFilter, MergeForEach, NewPartitionFilterOptimizer, PushDownForEachFlatten, PushUpFilter, SplitFilter, StreamTypeCastInserter], RULES_DISABLED=[FilterLogicExpressionSimplifier, PartitionFilterOptimizer]}\n",
      "2018-02-18 10:07:38,081 [main] INFO  org.apache.pig.newplan.logical.rules.ColumnPruneVisitor - Columns pruned for data: $1, $2, $3, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15, $16, $17, $18\n",
      "2018-02-18 10:07:38,100 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.textoutputformat.separator is deprecated. Instead, use mapreduce.output.textoutputformat.separator\n",
      "2018-02-18 10:07:38,276 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler - File concatenation threshold: 100 optimistic? false\n",
      "2018-02-18 10:07:38,342 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size before optimization: 1\n",
      "2018-02-18 10:07:38,343 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size after optimization: 1\n",
      "2018-02-18 10:07:38,523 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "2018-02-18 10:07:39,230 [main] INFO  org.apache.pig.tools.pigstats.ScriptState - Pig script settings are added to the job\n",
      "2018-02-18 10:07:39,408 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.reduce.markreset.buffer.percent is deprecated. Instead, use mapreduce.reduce.markreset.buffer.percent\n",
      "2018-02-18 10:07:39,408 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3\n",
      "2018-02-18 10:07:39,408 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress\n",
      "2018-02-18 10:07:39,414 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Reduce phase detected, estimating # of required reducers.\n",
      "2018-02-18 10:07:39,416 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Using reducer estimator: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.InputSizeReducerEstimator\n",
      "2018-02-18 10:07:39,426 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.InputSizeReducerEstimator - BytesPerReducer=1000000000 maxReducers=999 totalInputFileSize=120109135\n",
      "2018-02-18 10:07:39,427 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting Parallelism to 1\n",
      "2018-02-18 10:07:39,427 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "2018-02-18 10:07:41,404 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - creating jar file Job1698271013046544848.jar\n",
      "2018-02-18 10:07:44,894 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - jar file Job1698271013046544848.jar created\n",
      "2018-02-18 10:07:44,894 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.jar is deprecated. Instead, use mapreduce.job.jar\n",
      "2018-02-18 10:07:44,918 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting up single store job\n",
      "2018-02-18 10:07:44,931 [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Key [pig.schematuple] is false, will not generate code.\n",
      "2018-02-18 10:07:44,931 [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Starting process to move generated code to distributed cache\n",
      "2018-02-18 10:07:44,931 [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Setting key [pig.schematuple.classes] with classes to deserialize []\n",
      "2018-02-18 10:07:45,085 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 1 map-reduce job(s) waiting for submission.\n",
      "2018-02-18 10:07:45,086 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.tracker.http.address is deprecated. Instead, use mapreduce.jobtracker.http.address\n",
      "2018-02-18 10:07:45,103 [JobControl] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "2018-02-18 10:07:45,136 [JobControl] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-18 10:07:46,299 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths to process : 1\n",
      "2018-02-18 10:07:46,300 [JobControl] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths to process : 1\n",
      "2018-02-18 10:07:46,345 [JobControl] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths (combined) to process : 1\n",
      "2018-02-18 10:07:46,478 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1\n",
      "2018-02-18 10:07:46,764 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_1518944122177_0002\n",
      "2018-02-18 10:07:47,253 [JobControl] INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl - Submitted application application_1518944122177_0002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-18 10:07:47,365 [JobControl] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://yarnmaster:8088/proxy/application_1518944122177_0002/\n",
      "2018-02-18 10:07:47,365 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - HadoopJobId: job_1518944122177_0002\n",
      "2018-02-18 10:07:47,365 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Processing aliases 1-1,cleandata,cleandata_filtered,data,index,pairs,sorted_bag,sorted_index,word_groups,words_data,words_data_filtered\n",
      "2018-02-18 10:07:47,366 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - detailed locations: M: data[8,4],cleandata[-1,-1],cleandata_filtered[25,21],words_data[28,13],words_data_filtered[29,22],word_groups[32,14] C:  R: index[35,8],1-1[36,21],pairs[36,12],1-1[36,21],pairs[36,12],sorted_index[42,15],sorted_bag[43,17]\n",
      "2018-02-18 10:07:47,533 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 0% complete\n",
      "2018-02-18 10:08:13,522 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 5% complete\n",
      "2018-02-18 10:08:22,548 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 10% complete\n",
      "2018-02-18 10:08:28,238 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 14% complete\n",
      "2018-02-18 10:08:37,153 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 20% complete\n",
      "2018-02-18 10:08:46,196 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 25% complete\n",
      "2018-02-18 10:08:55,696 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 30% complete\n",
      "2018-02-18 10:09:04,695 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 35% complete\n",
      "2018-02-18 10:09:07,481 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 39% complete\n",
      "2018-02-18 10:09:10,761 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 44% complete\n",
      "2018-02-18 10:09:13,505 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 50% complete\n",
      "2018-02-18 10:09:33,387 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 83% complete\n",
      "2018-02-18 10:09:57,580 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 87% complete\n",
      "2018-02-18 10:10:31,015 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 91% complete\n",
      "2018-02-18 10:11:01,380 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 96% complete\n",
      "2018-02-18 10:11:33,277 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2018-02-18 10:11:41,725 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 100% complete\n",
      "2018-02-18 10:11:41,757 [main] INFO  org.apache.pig.tools.pigstats.SimplePigStats - Script Statistics: \n",
      "\n",
      "HadoopVersion\tPigVersion\tUserId\tStartedAt\tFinishedAt\tFeatures\n",
      "2.6.0-cdh5.9.0\t0.12.0-cdh5.9.0\troot\t2018-02-18 10:07:39\t2018-02-18 10:11:41\tGROUP_BY,FILTER\n",
      "\n",
      "Success!\n",
      "\n",
      "Job Stats (time in seconds):\n",
      "JobId\tMaps\tReduces\tMaxMapTime\tMinMapTIme\tAvgMapTime\tMedianMapTime\tMaxReduceTime\tMinReduceTime\tAvgReduceTime\tMedianReducetime\tAlias\tFeature\tOutputs\n",
      "job_1518944122177_0002\t1\t1\t75\t75\t75\t75\t127\t127\t127\t127\t1-1,cleandata,cleandata_filtered,data,index,pairs,sorted_bag,sorted_index,word_groups,words_data,words_data_filtered\tGROUP_BY,DISTINCT\thdfs://namenode:8020/user/root/inverted_index,\n",
      "\n",
      "Input(s):\n",
      "Successfully read 204617 records (120109499 bytes) from: \"hdfs://namenode:8020/user/root/forum_node.tsv\"\n",
      "\n",
      "Output(s):\n",
      "Successfully stored 185540 records (86936662 bytes) in: \"hdfs://namenode:8020/user/root/inverted_index\"\n",
      "\n",
      "Counters:\n",
      "Total records written : 185540\n",
      "Total bytes written : 86936662\n",
      "Spillable Memory Manager spill count : 0\n",
      "Total bags proactively spilled: 0\n",
      "Total records proactively spilled: 0\n",
      "\n",
      "Job DAG:\n",
      "job_1518944122177_0002\n",
      "\n",
      "\n",
      "2018-02-18 10:11:42,308 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Success!\n"
     ]
    }
   ],
   "source": [
    "! pig -f students-inverted-index.pig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   3 root supergroup          0 2018-02-18 10:11 inverted_index/_SUCCESS\r\n",
      "-rw-r--r--   3 root supergroup   86936662 2018-02-18 10:11 inverted_index/part-r-00000\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls inverted_index/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "googlecode\t{(79),(10617),(28844),(45630),(1006011),(1034790),(3001942),(5001673),(5002485),(6001946),(6002641),(6002651),(6002660),(6002690),(6003146),(6003978),(6004231),(6004298),(6005746),(6006014),(6007830),(6010307),(6015242),(6016801),(6022761),(6025168),(6027118),(7006210)}\t28\r\n",
      "googlemail\t{(66818),(66848),(67057),(6005844),(6006822),(6008744)}\t6\r\n",
      "googlemale\t{(10011506)}\t1\r\n",
      "googleplex\t{(2009842)}\t1\r\n",
      "googleplus\t{(6002796)}\t1\r\n",
      "googolplex\t{(2009965)}\t1\r\n",
      "goosebumps\t{(5002139)}\t1\r\n",
      "gorchynski\t{(12002258),(12002259),(12002270),(12002296),(12002300),(12002345),(12002440)}\t7\r\n",
      "gordeychuk\t{(12003403)}\t1\r\n",
      "gorilla834\t{(6019443),(6019538)}\t2\r\n",
      "tail: write error: Broken pipe\r\n",
      "tail: write error\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -cat inverted_index/part-r-00000 | tail -60000 | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
