{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pig: Índice invertido\n",
    "\n",
    "Partiendo del dataset de posts utilizado anteriormente, vamos a calcular un índice invertido. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/notebooks/pig-indiceinvertido\r\n"
     ]
    }
   ],
   "source": [
    "! rm -fr pig-indiceinvertido\n",
    "! mkdir -p pig-indiceinvertido\n",
    "import os\n",
    "os.chdir(\"pig-indiceinvertido/\")\n",
    "! pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso opcional - Instalación de depencias\n",
    "* Instalamos dos2unix para limpiar el fichero y convertirlo de formato DOS a Unix\n",
    "* Instalamos pig para ejecutar los correspondientes comandos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded plugins: fastestmirror, ovl\n",
      "Loading mirror speeds from cached hostfile\n",
      " * base: ftp.uma.es\n",
      " * epel: ftp.uma.es\n",
      " * extras: ftp.uma.es\n",
      " * updates: ftp.uma.es\n",
      "Package dos2unix-6.0.3-7.el7.x86_64 already installed and latest version\n",
      "Package pig-0.12.0+cdh5.9.0+95-1.cdh5.9.0.p0.30.el7.noarch already installed and latest version\n",
      "Package hbase-1.2.0+cdh5.9.0+205-1.cdh5.9.0.p0.30.el7.x86_64 already installed and latest version\n",
      "Nothing to do\n"
     ]
    }
   ],
   "source": [
    "! yum install -y dos2unix pig hbase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copiamos los ficheros de datos al directorio de trabajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 38M\r\n",
      "-rw-r--r-- 1 root root 1.8K Feb  5 15:33 forum1.tsv\r\n",
      "-rwxr-xr-x 1 root root  38M Feb  5 15:33 forum_node.tsv.gz\r\n"
     ]
    }
   ],
   "source": [
    "! cp ../dataset/forum_node.tsv.gz ../dataset/forum1.tsv .\n",
    "! ls -lh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descomprimimos el primer fichero y lo limpiamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dos2unix: converting file forum_node.tsv to Unix format ...\r\n"
     ]
    }
   ],
   "source": [
    "! gzip -d forum_node.tsv.gz && dos2unix -f forum_node.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el directorio de usuario en Hadoop si no existiera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -rm -r /user/$(whoami)/pig-indiceinvertido\n",
    "! hadoop fs -mkdir -p /user/$(whoami)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copiamos los ficheros a Hadoop y al directorio local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "! hadoop fs -put -p forum_node.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "! hadoop fs -put forum1.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   3 root supergroup       1774 2018-02-05 15:34 forum1.tsv\r\n",
      "-rwxr-xr-x   3 root root        120109135 2018-02-05 15:33 forum_node.tsv\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing students-inverted-index.pig\n"
     ]
    }
   ],
   "source": [
    "%%writefile students-inverted-index.pig\n",
    "\n",
    "/* 1.Carga el fichero de los posts forum_node.tsv, utilizando una extension de Piggybank para poder quitar la cabecera,\n",
    "en vez de usar directamente el PigStorage. */\n",
    "REGISTER /usr/lib/pig/piggybank.jar;\n",
    "DEFINE StringToInt InvokeForInt('java.lang.Integer.valueOf', 'String');\n",
    "\n",
    "data =\n",
    "    load 'forum_node.tsv'\n",
    "    using org.apache.pig.piggybank.storage.CSVExcelStorage('\\t', 'YES_MULTILINE', 'NOCHANGE', 'SKIP_INPUT_HEADER')\n",
    "    as (pid:chararray, title:chararray, tagnames:chararray,\n",
    "        author_id:chararray,body:chararray,\n",
    "        node_type:chararray, parent_id:chararray,\n",
    "        abs_parent_id:chararray,added_at:chararray,\n",
    "        score:chararray, state_string:chararray, last_edited_id:chararray,\n",
    "        last_activity_by_id:chararray, last_activity_at:chararray,\n",
    "        active_revision_id:chararray, extra:chararray,\n",
    "        extra_ref_id:chararray, extra_count:chararray, marked:chararray);\n",
    "\n",
    "/* 2.Limpiamos el fichero quitando los saltos de linea, expresiones html y la expresión regular que se proponia en el ejercicio. */\n",
    "cleandata = foreach data generate\n",
    "    REPLACE(pid, '[a-zA-Z]+', '') as post_id,\n",
    "    LOWER(REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(body, '\\\\\\\\n\\\\\\\\r', ''), '\\\\\\\\r', ''), '\\\\\\\\n', ''), '<*>', ''), '[^a-zA-Z0-9\\'\\\\s]+', ' ')) AS clean_body;\n",
    "\n",
    "/* 3.Filtramos los datos de post_id que no son numericos. */\n",
    "cleandata_filtered = filter cleandata by org.apache.pig.piggybank.evaluation.IsNumeric(post_id);\n",
    "\n",
    "/* 4.Creamos tuplas separando el body por espacios y convirtiendo el post_id en un numerico a través de una función custom, para evitar problemas que sufrimos con el cast de String a Integer. */\n",
    "words_data = FOREACH cleandata_filtered GENERATE StringToInt(post_id) as post_id_int:int, FLATTEN(TOKENIZE(clean_body)) as word;\n",
    "words_data_filtered = filter words_data by SIZE(word) > 0;\n",
    "\n",
    "/* 5.Agrupamos por palabra */\n",
    "word_groups = GROUP words_data_filtered BY word;\n",
    "\n",
    "/* 6.Por cada grupo de palabras, hacemos un distinct para los post_id, eliminando los duplicados, contamos el número de post en que aparece (despues de quitar los duplicados) y generamos una fila con el índice. */\n",
    "index = FOREACH word_groups {\n",
    "    pairs = DISTINCT $1.$0;\n",
    "    cnt = COUNT(pairs);\n",
    "    GENERATE $0 as word, pairs as index_bag, cnt as count;\n",
    "};\n",
    "\n",
    "/* 7.Como se pide que el indice lleve el post_id ordenador, ordenamos la bag resultante de los posts por su id. */\n",
    "sorted_index = foreach index {\n",
    "    sorted_bag = order index_bag by $0;\n",
    "    generate word, sorted_bag, count;\n",
    "}\n",
    "\n",
    "/* 8. Lo guardamos en un fichero. */\n",
    "STORE sorted_index INTO 'inverted_index';\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log4j:WARN No appenders could be found for logger (org.apache.hadoop.util.Shell).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n",
      "2018-02-05 15:34:48,014 [main] INFO  org.apache.pig.Main - Apache Pig version 0.12.0-cdh5.9.0 (rUnversioned directory) compiled Oct 21 2016, 01:17:18\n",
      "2018-02-05 15:34:48,015 [main] INFO  org.apache.pig.Main - Logging error messages to: /media/notebooks/pig-indiceinvertido/pig_1517844887974.log\n",
      "2018-02-05 15:34:48,044 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "2018-02-05 15:34:48,486 [main] INFO  org.apache.pig.impl.util.Utils - Default bootup file /root/.pigbootup not found\n",
      "2018-02-05 15:34:48,601 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-05 15:34:48,601 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "2018-02-05 15:34:48,602 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: file:///\n",
      "2018-02-05 15:34:48,689 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-05 15:34:48,777 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-05 15:34:48,850 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-05 15:34:48,900 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-05 15:34:48,977 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-05 15:34:49,113 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-05 15:34:49,172 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-05 15:34:49,237 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-05 15:34:49,364 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-05 15:34:50,497 [main] WARN  org.apache.pig.PigServer - Encountered Warning IMPLICIT_CAST_TO_LONG 1 time(s).\n",
      "2018-02-05 15:34:50,531 [main] INFO  org.apache.pig.tools.pigstats.ScriptState - Pig features used in the script: GROUP_BY,FILTER\n",
      "2018-02-05 15:34:50,624 [main] INFO  org.apache.pig.newplan.logical.optimizer.LogicalPlanOptimizer - {RULES_ENABLED=[AddForEach, ColumnMapKeyPrune, DuplicateForEachColumnRewrite, GroupByConstParallelSetter, ImplicitSplitInserter, LimitOptimizer, LoadTypeCastInserter, MergeFilter, MergeForEach, NewPartitionFilterOptimizer, PushDownForEachFlatten, PushUpFilter, SplitFilter, StreamTypeCastInserter], RULES_DISABLED=[FilterLogicExpressionSimplifier, PartitionFilterOptimizer]}\n",
      "2018-02-05 15:34:50,687 [main] INFO  org.apache.pig.newplan.logical.rules.ColumnPruneVisitor - Columns pruned for data: $1, $2, $3, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15, $16, $17, $18\n",
      "2018-02-05 15:34:50,712 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.textoutputformat.separator is deprecated. Instead, use mapreduce.output.textoutputformat.separator\n",
      "2018-02-05 15:34:50,893 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler - File concatenation threshold: 100 optimistic? false\n",
      "2018-02-05 15:34:50,955 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size before optimization: 1\n",
      "2018-02-05 15:34:50,955 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size after optimization: 1\n",
      "2018-02-05 15:34:51,027 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "2018-02-05 15:34:51,031 [main] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "2018-02-05 15:34:51,076 [main] INFO  org.apache.pig.tools.pigstats.ScriptState - Pig script settings are added to the job\n",
      "2018-02-05 15:34:51,194 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.reduce.markreset.buffer.percent is deprecated. Instead, use mapreduce.reduce.markreset.buffer.percent\n",
      "2018-02-05 15:34:51,194 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3\n",
      "2018-02-05 15:34:51,194 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress\n",
      "2018-02-05 15:34:51,209 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Reduce phase detected, estimating # of required reducers.\n",
      "2018-02-05 15:34:51,212 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Using reducer estimator: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.InputSizeReducerEstimator\n",
      "2018-02-05 15:34:51,218 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.InputSizeReducerEstimator - BytesPerReducer=1000000000 maxReducers=999 totalInputFileSize=120109135\n",
      "2018-02-05 15:34:51,218 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting Parallelism to 1\n",
      "2018-02-05 15:34:51,218 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "2018-02-05 15:34:51,249 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting up single store job\n",
      "2018-02-05 15:34:51,266 [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Key [pig.schematuple] is false, will not generate code.\n",
      "2018-02-05 15:34:51,266 [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Starting process to move generated code to distributed cache\n",
      "2018-02-05 15:34:51,266 [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Distributed cache not supported or needed in local mode. Setting key [pig.schematuple.local.dir] with code temp directory: /tmp/1517844891266-0\n",
      "2018-02-05 15:34:51,430 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 1 map-reduce job(s) waiting for submission.\n",
      "2018-02-05 15:34:51,432 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.tracker.http.address is deprecated. Instead, use mapreduce.jobtracker.http.address\n",
      "2018-02-05 15:34:51,466 [JobControl] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "2018-02-05 15:34:51,753 [JobControl] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n",
      "2018-02-05 15:34:51,810 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths to process : 1\n",
      "2018-02-05 15:34:51,810 [JobControl] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths to process : 1\n",
      "2018-02-05 15:34:51,863 [JobControl] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths (combined) to process : 4\n",
      "2018-02-05 15:34:51,929 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:4\n",
      "2018-02-05 15:34:51,951 [JobControl] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-05 15:34:52,126 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_local1109379629_0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-05 15:34:52,431 [JobControl] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://localhost:8080/\n",
      "2018-02-05 15:34:52,433 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - HadoopJobId: job_local1109379629_0001\n",
      "2018-02-05 15:34:52,435 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Processing aliases 1-1,cleandata,cleandata_filtered,data,index,pairs,sorted_bag,sorted_index,word_groups,words_data,words_data_filtered\n",
      "2018-02-05 15:34:52,436 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - detailed locations: M: data[8,4],cleandata[-1,-1],cleandata_filtered[25,21],words_data[28,13],words_data_filtered[29,22],word_groups[32,14] C:  R: index[35,8],1-1[36,21],pairs[36,12],1-1[36,21],pairs[36,12],sorted_index[42,15],sorted_bag[43,17]\n",
      "2018-02-05 15:34:52,451 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 0% complete\n",
      "2018-02-05 15:34:52,456 [Thread-5] INFO  org.apache.hadoop.mapred.LocalJobRunner - OutputCommitter set in config null\n",
      "2018-02-05 15:34:52,515 [Thread-5] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.textoutputformat.separator is deprecated. Instead, use mapreduce.output.textoutputformat.separator\n",
      "2018-02-05 15:34:52,517 [Thread-5] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "2018-02-05 15:34:52,517 [Thread-5] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.reduce.markreset.buffer.percent is deprecated. Instead, use mapreduce.reduce.markreset.buffer.percent\n",
      "2018-02-05 15:34:52,518 [Thread-5] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "2018-02-05 15:34:52,518 [Thread-5] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-05 15:34:52,519 [Thread-5] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "2018-02-05 15:34:52,523 [Thread-5] INFO  org.apache.hadoop.mapred.LocalJobRunner - OutputCommitter is org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter\n",
      "2018-02-05 15:34:52,619 [Thread-5] INFO  org.apache.hadoop.mapred.LocalJobRunner - Waiting for map tasks\n",
      "2018-02-05 15:34:52,621 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Starting task: attempt_local1109379629_0001_m_000000_0\n",
      "2018-02-05 15:34:52,743 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "2018-02-05 15:34:52,795 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task -  Using ResourceCalculatorProcessTree : [ ]\n",
      "2018-02-05 15:34:52,818 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Processing split: Number of splits :1\n",
      "Total Length = 33554432\n",
      "Input split[0]:\n",
      "   Length = 33554432\n",
      "  Locations:\n",
      "\n",
      "-----------------------\n",
      "\n",
      "2018-02-05 15:34:52,849 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader - Current split being processed file:/media/notebooks/pig-indiceinvertido/forum_node.tsv:0+33554432\n",
      "2018-02-05 15:34:52,946 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - (EQUATOR) 0 kvi 26214396(104857584)\n",
      "2018-02-05 15:34:52,946 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - mapreduce.task.io.sort.mb: 100\n",
      "2018-02-05 15:34:52,949 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - soft limit at 83886080\n",
      "2018-02-05 15:34:52,949 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 0; bufvoid = 104857600\n",
      "2018-02-05 15:34:52,949 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 26214396; length = 6553600\n",
      "2018-02-05 15:34:52,983 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "2018-02-05 15:34:53,288 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.data.SchemaTupleBackend - Key [pig.schematuple] was not set... will not generate code.\n",
      "2018-02-05 15:34:53,307 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Map - Aliases being processed per job phase (AliasName[line,offset]): M: data[8,4],cleandata[-1,-1],cleandata_filtered[25,21],words_data[28,13],words_data_filtered[29,22],word_groups[32,14] C:  R: index[35,8],1-1[36,21],pairs[36,12],1-1[36,21],pairs[36,12],sorted_index[42,15],sorted_bag[43,17]\n",
      "2018-02-05 15:34:58,790 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map\n",
      "2018-02-05 15:35:01,346 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Spilling map output\n",
      "2018-02-05 15:35:01,346 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 0; bufend = 39858232; bufvoid = 104857600\n",
      "2018-02-05 15:35:01,346 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 26214396(104857584); kvend = 15207436(60829744); length = 11006961/6553600\n",
      "2018-02-05 15:35:01,346 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - (EQUATOR) 50343980 kvi 12585988(50343952)\n",
      "2018-02-05 15:35:01,796 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map\n",
      "2018-02-05 15:35:04,798 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map\n",
      "2018-02-05 15:35:06,138 [SpillThread] INFO  org.apache.hadoop.mapred.MapTask - Finished spill 0\n",
      "2018-02-05 15:35:06,139 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - (RESET) equator 50343980 kv 12585988(50343952) kvi 9964564(39858256)\n",
      "2018-02-05 15:35:06,735 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map\n",
      "2018-02-05 15:35:06,736 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Starting flush of map output\n",
      "2018-02-05 15:35:06,736 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Spilling map output\n",
      "2018-02-05 15:35:06,736 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 50343980; bufend = 63284165; bufvoid = 104857600\n",
      "2018-02-05 15:35:06,736 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 12585988(50343952); kvend = 9125100(36500400); length = 3460889/6553600\n",
      "2018-02-05 15:35:07,764 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Finished spill 1\n",
      "2018-02-05 15:35:07,783 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Merger - Merging 2 sorted segments\n",
      "2018-02-05 15:35:07,800 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > sort > \n",
      "2018-02-05 15:35:07,804 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Merger - Down to the last merge-pass, with 2 segments left of total size: 60032390 bytes\n",
      "2018-02-05 15:35:10,517 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Task:attempt_local1109379629_0001_m_000000_0 is done. And is in the process of committing\n",
      "2018-02-05 15:35:10,519 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > sort\n",
      "2018-02-05 15:35:10,520 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Task 'attempt_local1109379629_0001_m_000000_0' done.\n",
      "2018-02-05 15:35:10,520 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Finishing task: attempt_local1109379629_0001_m_000000_0\n",
      "2018-02-05 15:35:10,520 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Starting task: attempt_local1109379629_0001_m_000001_0\n",
      "2018-02-05 15:35:10,534 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "2018-02-05 15:35:10,535 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task -  Using ResourceCalculatorProcessTree : [ ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-05 15:35:10,554 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Processing split: Number of splits :1\n",
      "Total Length = 33554432\n",
      "Input split[0]:\n",
      "   Length = 33554432\n",
      "  Locations:\n",
      "\n",
      "-----------------------\n",
      "\n",
      "2018-02-05 15:35:10,574 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader - Current split being processed file:/media/notebooks/pig-indiceinvertido/forum_node.tsv:33554432+33554432\n",
      "2018-02-05 15:35:10,728 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - (EQUATOR) 0 kvi 26214396(104857584)\n",
      "2018-02-05 15:35:10,729 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - mapreduce.task.io.sort.mb: 100\n",
      "2018-02-05 15:35:10,752 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - soft limit at 83886080\n",
      "2018-02-05 15:35:10,752 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 0; bufvoid = 104857600\n",
      "2018-02-05 15:35:10,752 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 26214396; length = 6553600\n",
      "2018-02-05 15:35:10,780 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "2018-02-05 15:35:10,872 [LocalJobRunner Map Task Executor #0] WARN  org.apache.pig.data.SchemaTupleBackend - SchemaTupleBackend has already been initialized\n",
      "2018-02-05 15:35:10,931 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Map - Aliases being processed per job phase (AliasName[line,offset]): M: data[8,4],cleandata[-1,-1],cleandata_filtered[25,21],words_data[28,13],words_data_filtered[29,22],word_groups[32,14] C:  R: index[35,8],1-1[36,21],pairs[36,12],1-1[36,21],pairs[36,12],sorted_index[42,15],sorted_bag[43,17]\n",
      "2018-02-05 15:35:16,548 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map\n",
      "2018-02-05 15:35:18,821 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Spilling map output\n",
      "2018-02-05 15:35:18,821 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 0; bufend = 40782076; bufvoid = 104857600\n",
      "2018-02-05 15:35:18,821 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 26214396(104857584); kvend = 15438396(61753584); length = 10776001/6553600\n",
      "2018-02-05 15:35:18,821 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - (EQUATOR) 51267822 kvi 12816948(51267792)\n",
      "2018-02-05 15:35:19,551 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map\n",
      "2018-02-05 15:35:22,552 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map\n",
      "2018-02-05 15:35:22,989 [SpillThread] INFO  org.apache.hadoop.mapred.MapTask - Finished spill 0\n",
      "2018-02-05 15:35:22,989 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - (RESET) equator 51267822 kv 12816948(51267792) kvi 10195524(40782096)\n",
      "2018-02-05 15:35:24,578 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map\n",
      "2018-02-05 15:35:24,578 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Starting flush of map output\n",
      "2018-02-05 15:35:24,579 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Spilling map output\n",
      "2018-02-05 15:35:24,579 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 51267822; bufend = 69773215; bufvoid = 104857600\n",
      "2018-02-05 15:35:24,579 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 12816948(51267792); kvend = 7966940(31867760); length = 4850009/6553600\n",
      "2018-02-05 15:35:25,554 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > sort\n",
      "2018-02-05 15:35:26,236 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Finished spill 1\n",
      "2018-02-05 15:35:26,244 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Merger - Merging 2 sorted segments\n",
      "2018-02-05 15:35:26,245 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Merger - Down to the last merge-pass, with 2 segments left of total size: 67100520 bytes\n",
      "2018-02-05 15:35:28,556 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > sort > \n",
      "2018-02-05 15:35:28,752 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Task:attempt_local1109379629_0001_m_000001_0 is done. And is in the process of committing\n",
      "2018-02-05 15:35:28,753 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > sort\n",
      "2018-02-05 15:35:28,753 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Task 'attempt_local1109379629_0001_m_000001_0' done.\n",
      "2018-02-05 15:35:28,753 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Finishing task: attempt_local1109379629_0001_m_000001_0\n",
      "2018-02-05 15:35:28,753 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Starting task: attempt_local1109379629_0001_m_000002_0\n",
      "2018-02-05 15:35:28,758 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "2018-02-05 15:35:28,764 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task -  Using ResourceCalculatorProcessTree : [ ]\n",
      "2018-02-05 15:35:28,768 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Processing split: Number of splits :1\n",
      "Total Length = 33554432\n",
      "Input split[0]:\n",
      "   Length = 33554432\n",
      "  Locations:\n",
      "\n",
      "-----------------------\n",
      "\n",
      "2018-02-05 15:35:28,772 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader - Current split being processed file:/media/notebooks/pig-indiceinvertido/forum_node.tsv:67108864+33554432\n",
      "2018-02-05 15:35:28,789 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - (EQUATOR) 0 kvi 26214396(104857584)\n",
      "2018-02-05 15:35:28,789 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - mapreduce.task.io.sort.mb: 100\n",
      "2018-02-05 15:35:28,789 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - soft limit at 83886080\n",
      "2018-02-05 15:35:28,789 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 0; bufvoid = 104857600\n",
      "2018-02-05 15:35:28,789 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 26214396; length = 6553600\n",
      "2018-02-05 15:35:28,790 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "2018-02-05 15:35:28,812 [LocalJobRunner Map Task Executor #0] WARN  org.apache.pig.data.SchemaTupleBackend - SchemaTupleBackend has already been initialized\n",
      "2018-02-05 15:35:28,846 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Map - Aliases being processed per job phase (AliasName[line,offset]): M: data[8,4],cleandata[-1,-1],cleandata_filtered[25,21],words_data[28,13],words_data_filtered[29,22],word_groups[32,14] C:  R: index[35,8],1-1[36,21],pairs[36,12],1-1[36,21],pairs[36,12],sorted_index[42,15],sorted_bag[43,17]\n",
      "2018-02-05 15:35:34,771 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map\n",
      "2018-02-05 15:35:36,324 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Spilling map output\n",
      "2018-02-05 15:35:36,324 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 0; bufend = 40677656; bufvoid = 104857600\n",
      "2018-02-05 15:35:36,324 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 26214396(104857584); kvend = 15412296(61649184); length = 10802101/6553600\n",
      "2018-02-05 15:35:36,324 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - (EQUATOR) 51163412 kvi 12790848(51163392)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-05 15:35:37,773 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map\n",
      "2018-02-05 15:35:40,377 [SpillThread] INFO  org.apache.hadoop.mapred.MapTask - Finished spill 0\n",
      "2018-02-05 15:35:40,377 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - (RESET) equator 51163412 kv 12790848(51163392) kvi 10169420(40677680)\n",
      "2018-02-05 15:35:40,774 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map\n",
      "2018-02-05 15:35:42,068 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map\n",
      "2018-02-05 15:35:42,068 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Starting flush of map output\n",
      "2018-02-05 15:35:42,068 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Spilling map output\n",
      "2018-02-05 15:35:42,068 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 51163412; bufend = 70737927; bufvoid = 104857600\n",
      "2018-02-05 15:35:42,068 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 12790848(51163392); kvend = 7585048(30340192); length = 5205801/6553600\n",
      "2018-02-05 15:35:43,636 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Finished spill 1\n",
      "2018-02-05 15:35:43,637 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Merger - Merging 2 sorted segments\n",
      "2018-02-05 15:35:43,638 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Merger - Down to the last merge-pass, with 2 segments left of total size: 68256169 bytes\n",
      "2018-02-05 15:35:43,777 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > sort > \n",
      "2018-02-05 15:35:46,086 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Task:attempt_local1109379629_0001_m_000002_0 is done. And is in the process of committing\n",
      "2018-02-05 15:35:46,089 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > sort\n",
      "2018-02-05 15:35:46,089 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Task 'attempt_local1109379629_0001_m_000002_0' done.\n",
      "2018-02-05 15:35:46,089 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Finishing task: attempt_local1109379629_0001_m_000002_0\n",
      "2018-02-05 15:35:46,089 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Starting task: attempt_local1109379629_0001_m_000003_0\n",
      "2018-02-05 15:35:46,104 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "2018-02-05 15:35:46,105 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task -  Using ResourceCalculatorProcessTree : [ ]\n",
      "2018-02-05 15:35:46,111 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Processing split: Number of splits :1\n",
      "Total Length = 19445839\n",
      "Input split[0]:\n",
      "   Length = 19445839\n",
      "  Locations:\n",
      "\n",
      "-----------------------\n",
      "\n",
      "2018-02-05 15:35:46,115 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader - Current split being processed file:/media/notebooks/pig-indiceinvertido/forum_node.tsv:100663296+19445839\n",
      "2018-02-05 15:35:46,160 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - (EQUATOR) 0 kvi 26214396(104857584)\n",
      "2018-02-05 15:35:46,160 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - mapreduce.task.io.sort.mb: 100\n",
      "2018-02-05 15:35:46,160 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - soft limit at 83886080\n",
      "2018-02-05 15:35:46,160 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 0; bufvoid = 104857600\n",
      "2018-02-05 15:35:46,160 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 26214396; length = 6553600\n",
      "2018-02-05 15:35:46,164 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "2018-02-05 15:35:46,207 [LocalJobRunner Map Task Executor #0] WARN  org.apache.pig.data.SchemaTupleBackend - SchemaTupleBackend has already been initialized\n",
      "2018-02-05 15:35:46,235 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Map - Aliases being processed per job phase (AliasName[line,offset]): M: data[8,4],cleandata[-1,-1],cleandata_filtered[25,21],words_data[28,13],words_data_filtered[29,22],word_groups[32,14] C:  R: index[35,8],1-1[36,21],pairs[36,12],1-1[36,21],pairs[36,12],sorted_index[42,15],sorted_bag[43,17]\n",
      "2018-02-05 15:35:52,104 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map\n",
      "2018-02-05 15:35:54,063 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > map\n",
      "2018-02-05 15:35:54,063 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Starting flush of map output\n",
      "2018-02-05 15:35:54,063 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Spilling map output\n",
      "2018-02-05 15:35:54,063 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 0; bufend = 40451886; bufvoid = 104857600\n",
      "2018-02-05 15:35:54,063 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 26214396(104857584); kvend = 15404404(61617616); length = 10809993/6553600\n",
      "2018-02-05 15:35:55,106 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - map > sort\n",
      "2018-02-05 15:35:58,071 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Finished spill 0\n",
      "2018-02-05 15:35:58,073 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Task:attempt_local1109379629_0001_m_000003_0 is done. And is in the process of committing\n",
      "2018-02-05 15:35:58,075 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - map\n",
      "2018-02-05 15:35:58,075 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Task 'attempt_local1109379629_0001_m_000003_0' done.\n",
      "2018-02-05 15:35:58,075 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Finishing task: attempt_local1109379629_0001_m_000003_0\n",
      "2018-02-05 15:35:58,075 [Thread-5] INFO  org.apache.hadoop.mapred.LocalJobRunner - map task executor complete.\n",
      "2018-02-05 15:35:58,093 [Thread-5] INFO  org.apache.hadoop.mapred.LocalJobRunner - Waiting for reduce tasks\n",
      "2018-02-05 15:35:58,094 [pool-5-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - Starting task: attempt_local1109379629_0001_r_000000_0\n",
      "2018-02-05 15:35:58,135 [pool-5-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "2018-02-05 15:35:58,137 [pool-5-thread-1] INFO  org.apache.hadoop.mapred.Task -  Using ResourceCalculatorProcessTree : [ ]\n",
      "2018-02-05 15:35:58,150 [pool-5-thread-1] INFO  org.apache.hadoop.mapred.ReduceTask - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@76ce994c\n",
      "2018-02-05 15:35:58,202 [pool-5-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - MergerManager: memoryLimit=652528832, maxSingleShuffleLimit=163132208, mergeThreshold=430669056, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "2018-02-05 15:35:58,208 [EventFetcher for fetching Map Completion Events] INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher - attempt_local1109379629_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "2018-02-05 15:35:58,336 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.LocalFetcher - localfetcher#1 about to shuffle output of map attempt_local1109379629_0001_m_000002_0 decomp: 68256171 len: 68256175 to MEMORY\n",
      "2018-02-05 15:35:58,501 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput - Read 68256171 bytes from map-output for attempt_local1109379629_0001_m_000002_0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-05 15:35:58,515 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - closeInMemoryFile -> map-output of size: 68256171, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->68256171\n",
      "2018-02-05 15:35:58,575 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.LocalFetcher - localfetcher#1 about to shuffle output of map attempt_local1109379629_0001_m_000001_0 decomp: 67100522 len: 67100526 to MEMORY\n",
      "2018-02-05 15:35:58,891 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput - Read 67100522 bytes from map-output for attempt_local1109379629_0001_m_000001_0\n",
      "2018-02-05 15:35:58,891 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - closeInMemoryFile -> map-output of size: 67100522, inMemoryMapOutputs.size() -> 2, commitMemory -> 68256171, usedMemory ->135356693\n",
      "2018-02-05 15:35:59,050 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.LocalFetcher - localfetcher#1 about to shuffle output of map attempt_local1109379629_0001_m_000000_0 decomp: 60032392 len: 60032396 to MEMORY\n",
      "2018-02-05 15:35:59,407 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput - Read 60032392 bytes from map-output for attempt_local1109379629_0001_m_000000_0\n",
      "2018-02-05 15:35:59,408 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - closeInMemoryFile -> map-output of size: 60032392, inMemoryMapOutputs.size() -> 3, commitMemory -> 135356693, usedMemory ->195389085\n",
      "2018-02-05 15:35:59,419 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.LocalFetcher - localfetcher#1 about to shuffle output of map attempt_local1109379629_0001_m_000003_0 decomp: 45856899 len: 45856903 to MEMORY\n",
      "2018-02-05 15:35:59,499 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput - Read 45856899 bytes from map-output for attempt_local1109379629_0001_m_000003_0\n",
      "2018-02-05 15:35:59,500 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - closeInMemoryFile -> map-output of size: 45856899, inMemoryMapOutputs.size() -> 4, commitMemory -> 195389085, usedMemory ->241245984\n",
      "2018-02-05 15:35:59,503 [EventFetcher for fetching Map Completion Events] INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher - EventFetcher is interrupted.. Returning\n",
      "2018-02-05 15:35:59,504 [pool-5-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - 4 / 4 copied.\n",
      "2018-02-05 15:35:59,504 [pool-5-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - finalMerge called with 4 in-memory map-outputs and 0 on-disk map-outputs\n",
      "2018-02-05 15:35:59,506 [pool-5-thread-1] INFO  org.apache.hadoop.mapred.Merger - Merging 4 sorted segments\n",
      "2018-02-05 15:35:59,507 [pool-5-thread-1] INFO  org.apache.hadoop.mapred.Merger - Down to the last merge-pass, with 4 segments left of total size: 241245960 bytes\n",
      "2018-02-05 15:36:04,142 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > sort\n",
      "2018-02-05 15:36:04,933 [pool-5-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - Merged 4 segments, 241245984 bytes to disk to satisfy reduce memory limit\n",
      "2018-02-05 15:36:04,933 [pool-5-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - Merging 1 files, 241245982 bytes from disk\n",
      "2018-02-05 15:36:04,936 [pool-5-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - Merging 0 segments, 0 bytes from memory into reduce\n",
      "2018-02-05 15:36:04,936 [pool-5-thread-1] INFO  org.apache.hadoop.mapred.Merger - Merging 1 sorted segments\n",
      "2018-02-05 15:36:04,938 [pool-5-thread-1] INFO  org.apache.hadoop.mapred.Merger - Down to the last merge-pass, with 1 segments left of total size: 241245972 bytes\n",
      "2018-02-05 15:36:04,938 [pool-5-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > sort\n",
      "2018-02-05 15:36:04,957 [pool-5-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "2018-02-05 15:36:04,982 [pool-5-thread-1] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "2018-02-05 15:36:05,011 [pool-5-thread-1] WARN  org.apache.pig.data.SchemaTupleBackend - SchemaTupleBackend has already been initialized\n",
      "2018-02-05 15:36:05,034 [pool-5-thread-1] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce - Aliases being processed per job phase (AliasName[line,offset]): M: data[8,4],cleandata[-1,-1],cleandata_filtered[25,21],words_data[28,13],words_data_filtered[29,22],word_groups[32,14] C:  R: index[35,8],1-1[36,21],pairs[36,12],1-1[36,21],pairs[36,12],sorted_index[42,15],sorted_bag[43,17]\n",
      "2018-02-05 15:36:07,149 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-05 15:36:10,150 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-05 15:36:13,151 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-05 15:36:16,152 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-05 15:36:19,214 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-05 15:36:22,216 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-05 15:36:25,218 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-05 15:36:28,220 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-05 15:36:31,237 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-05 15:36:34,239 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-05 15:36:37,239 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-05 15:36:40,480 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-05 15:36:43,480 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-05 15:36:46,481 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-05 15:36:49,483 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-05 15:36:52,484 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-05 15:36:55,485 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-05 15:36:58,488 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-05 15:37:01,493 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-05 15:37:04,495 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-05 15:37:07,497 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-05 15:37:10,498 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-05 15:37:13,502 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-05 15:37:16,504 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-05 15:37:19,506 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-05 15:37:22,507 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-05 15:37:25,511 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-05 15:37:28,512 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-05 15:37:31,514 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-05 15:37:34,518 [communication thread] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-05 15:37:36,038 [pool-5-thread-1] INFO  org.apache.hadoop.mapred.Task - Task:attempt_local1109379629_0001_r_000000_0 is done. And is in the process of committing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-05 15:37:36,050 [pool-5-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-05 15:37:36,050 [pool-5-thread-1] INFO  org.apache.hadoop.mapred.Task - Task attempt_local1109379629_0001_r_000000_0 is allowed to commit now\n",
      "2018-02-05 15:37:36,063 [pool-5-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_local1109379629_0001_r_000000_0' to file:/media/notebooks/pig-indiceinvertido/inverted_index/_temporary/0/task_local1109379629_0001_r_000000\n",
      "2018-02-05 15:37:36,064 [pool-5-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2018-02-05 15:37:36,064 [pool-5-thread-1] INFO  org.apache.hadoop.mapred.Task - Task 'attempt_local1109379629_0001_r_000000_0' done.\n",
      "2018-02-05 15:37:36,065 [pool-5-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - Finishing task: attempt_local1109379629_0001_r_000000_0\n",
      "2018-02-05 15:37:36,065 [Thread-5] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce task executor complete.\n",
      "2018-02-05 15:37:36,560 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "2018-02-05 15:37:36,560 [main] WARN  org.apache.pig.tools.pigstats.PigStatsUtil - Failed to get RunningJob for job job_local1109379629_0001\n",
      "2018-02-05 15:37:36,573 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 100% complete\n",
      "2018-02-05 15:37:36,573 [main] INFO  org.apache.pig.tools.pigstats.SimplePigStats - Detected Local mode. Stats reported below may be incomplete\n",
      "2018-02-05 15:37:36,591 [main] INFO  org.apache.pig.tools.pigstats.SimplePigStats - Script Statistics: \n",
      "\n",
      "HadoopVersion\tPigVersion\tUserId\tStartedAt\tFinishedAt\tFeatures\n",
      "2.6.0-cdh5.9.0\t0.12.0-cdh5.9.0\troot\t2018-02-05 15:34:51\t2018-02-05 15:37:36\tGROUP_BY,FILTER\n",
      "\n",
      "Success!\n",
      "\n",
      "Job Stats (time in seconds):\n",
      "JobId\tAlias\tFeature\tOutputs\n",
      "job_local1109379629_0001\t1-1,cleandata,cleandata_filtered,data,index,pairs,sorted_bag,sorted_index,word_groups,words_data,words_data_filtered\tGROUP_BY,DISTINCT\tfile:///media/notebooks/pig-indiceinvertido/inverted_index,\n",
      "\n",
      "Input(s):\n",
      "Successfully read records from: \"file:///media/notebooks/pig-indiceinvertido/forum_node.tsv\"\n",
      "\n",
      "Output(s):\n",
      "Successfully stored records in: \"file:///media/notebooks/pig-indiceinvertido/inverted_index\"\n",
      "\n",
      "Job DAG:\n",
      "job_local1109379629_0001\n",
      "\n",
      "\n",
      "2018-02-05 15:37:36,593 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Success!\n"
     ]
    }
   ],
   "source": [
    "! pig -f students-inverted-index.pig -x local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t{(2010230)}\t1\r\n",
      "reciprocals\t{(2009620),(2009707),(2010947),(2014292),(9002395)}\t5\r\n",
      "reciprocate\t{(2001902),(6004447)}\t2\r\n",
      "reciprocity\t{(2008341),(2009563),(2009571),(2009804),(2010292)}\t5\r\n",
      "recitations\t{(2018236),(5014249)}\t2\r\n",
      "reclamation\t{(10438)}\t1\r\n",
      "recliningon\t{(6022210)}\t1\r\n",
      "recognisers\t{(5007198),(5007700),(5007756)}\t3\r\n",
      "recognising\t{(1014977),(1034000),(6015555),(8000513)}\t4\r\n",
      "recognition\t{(3982),(5301),(8066),(9102),(22789),(41152),(47260),(51450),(51559),(52330),(53802),(53881),(53962),(60657),(60916),(63420),(64321),(64470),(66804),(66874),(67092),(67121),(67242),(67488),(1000219),(1000901),(1001733),(1001920),(1002371),(1005129),(1006927),(1007272),(1008146),(1008955),(1008996),(1009010),(1009776),(1010107),(1010329),(1010351),(1012085),(1012836),(1013518),(1013848),(1014107),(1015692),(1018390),(1023592),(1025649),(1026946),(1028196),(1030214),(1030460),(1030646),(1030651),(1031191),(1031238),(1031734),(1032697),(1032720),(1033371),(1033635),(1033857),(1034081),(1034451),(1034946),(1034954),(1035052),(1035056),(1035061),(1035176),(1035289),(2001357),(2006024),(2013591),(2016523),(2016895),(3000200),(3000463),(3001489),(3001492),(3001645),(3001837),(5004737),(5011243),(5014578),(5014587),(6000004),(6021164),(6031048),(7002749),(7003833),(7007679),(8001664),(8002598),(8004135),(8005177),(10000159),(10000366),(10000718),(10001470),(10002665),(10003051),(10003285),(10003312),(10005751),(10006251),(10006449),(10006944),(10007030),(10007571),(10007850),(10010554),(10010641),(10011519),(12003284),(12003362)}\t117\r\n",
      "tail: write error: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "! tail -40000 ./inverted_index/part-r-00000  | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log4j:WARN No appenders could be found for logger (org.apache.hadoop.util.Shell).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n",
      "2018-02-05 15:38:35,802 [main] INFO  org.apache.pig.Main - Apache Pig version 0.12.0-cdh5.9.0 (rUnversioned directory) compiled Oct 21 2016, 01:17:18\n",
      "2018-02-05 15:38:35,803 [main] INFO  org.apache.pig.Main - Logging error messages to: /media/notebooks/pig-indiceinvertido/pig_1517845115761.log\n",
      "2018-02-05 15:38:37,075 [main] INFO  org.apache.pig.impl.util.Utils - Default bootup file /root/.pigbootup not found\n",
      "2018-02-05 15:38:37,333 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "2018-02-05 15:38:37,333 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-05 15:38:37,333 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: hdfs://namenode:8020\n",
      "2018-02-05 15:38:38,461 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-05 15:38:38,548 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-05 15:38:38,646 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-05 15:38:38,719 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-05 15:38:38,795 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-05 15:38:38,891 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-05 15:38:38,992 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-05 15:38:39,063 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-05 15:38:39,199 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-05 15:38:40,730 [main] WARN  org.apache.pig.PigServer - Encountered Warning IMPLICIT_CAST_TO_LONG 1 time(s).\n",
      "2018-02-05 15:38:40,760 [main] INFO  org.apache.pig.tools.pigstats.ScriptState - Pig features used in the script: GROUP_BY,FILTER\n",
      "2018-02-05 15:38:40,855 [main] INFO  org.apache.pig.newplan.logical.optimizer.LogicalPlanOptimizer - {RULES_ENABLED=[AddForEach, ColumnMapKeyPrune, DuplicateForEachColumnRewrite, GroupByConstParallelSetter, ImplicitSplitInserter, LimitOptimizer, LoadTypeCastInserter, MergeFilter, MergeForEach, NewPartitionFilterOptimizer, PushDownForEachFlatten, PushUpFilter, SplitFilter, StreamTypeCastInserter], RULES_DISABLED=[FilterLogicExpressionSimplifier, PartitionFilterOptimizer]}\n",
      "2018-02-05 15:38:40,912 [main] INFO  org.apache.pig.newplan.logical.rules.ColumnPruneVisitor - Columns pruned for data: $1, $2, $3, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15, $16, $17, $18\n",
      "2018-02-05 15:38:40,933 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.textoutputformat.separator is deprecated. Instead, use mapreduce.output.textoutputformat.separator\n",
      "2018-02-05 15:38:41,112 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler - File concatenation threshold: 100 optimistic? false\n",
      "2018-02-05 15:38:41,176 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size before optimization: 1\n",
      "2018-02-05 15:38:41,176 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size after optimization: 1\n",
      "2018-02-05 15:38:41,358 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "2018-02-05 15:38:41,947 [main] INFO  org.apache.pig.tools.pigstats.ScriptState - Pig script settings are added to the job\n",
      "2018-02-05 15:38:42,072 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.reduce.markreset.buffer.percent is deprecated. Instead, use mapreduce.reduce.markreset.buffer.percent\n",
      "2018-02-05 15:38:42,072 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3\n",
      "2018-02-05 15:38:42,072 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress\n",
      "2018-02-05 15:38:42,078 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Reduce phase detected, estimating # of required reducers.\n",
      "2018-02-05 15:38:42,085 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Using reducer estimator: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.InputSizeReducerEstimator\n",
      "2018-02-05 15:38:42,091 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.InputSizeReducerEstimator - BytesPerReducer=1000000000 maxReducers=999 totalInputFileSize=120109135\n",
      "2018-02-05 15:38:42,091 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting Parallelism to 1\n",
      "2018-02-05 15:38:42,091 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "2018-02-05 15:38:44,043 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - creating jar file Job5391609132411774153.jar\n",
      "2018-02-05 15:38:47,668 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - jar file Job5391609132411774153.jar created\n",
      "2018-02-05 15:38:47,668 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.jar is deprecated. Instead, use mapreduce.job.jar\n",
      "2018-02-05 15:38:47,700 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting up single store job\n",
      "2018-02-05 15:38:47,719 [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Key [pig.schematuple] is false, will not generate code.\n",
      "2018-02-05 15:38:47,719 [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Starting process to move generated code to distributed cache\n",
      "2018-02-05 15:38:47,720 [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Setting key [pig.schematuple.classes] with classes to deserialize []\n",
      "2018-02-05 15:38:47,901 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 1 map-reduce job(s) waiting for submission.\n",
      "2018-02-05 15:38:47,902 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.tracker.http.address is deprecated. Instead, use mapreduce.jobtracker.http.address\n",
      "2018-02-05 15:38:47,923 [JobControl] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at yarnmaster/172.18.0.2:8032\n",
      "2018-02-05 15:38:47,978 [JobControl] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n",
      "2018-02-05 15:38:48,837 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths to process : 1\n",
      "2018-02-05 15:38:48,837 [JobControl] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths to process : 1\n",
      "2018-02-05 15:38:48,883 [JobControl] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths (combined) to process : 1\n",
      "2018-02-05 15:38:49,027 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1\n",
      "2018-02-05 15:38:49,270 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_1517830644613_0003\n",
      "2018-02-05 15:38:49,780 [JobControl] INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl - Submitted application application_1517830644613_0003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-05 15:38:49,933 [JobControl] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://yarnmaster:8088/proxy/application_1517830644613_0003/\n",
      "2018-02-05 15:38:49,934 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - HadoopJobId: job_1517830644613_0003\n",
      "2018-02-05 15:38:49,934 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Processing aliases 1-1,cleandata,cleandata_filtered,data,index,pairs,sorted_bag,sorted_index,word_groups,words_data,words_data_filtered\n",
      "2018-02-05 15:38:49,934 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - detailed locations: M: data[8,4],cleandata[-1,-1],cleandata_filtered[25,21],words_data[28,13],words_data_filtered[29,22],word_groups[32,14] C:  R: index[35,8],1-1[36,21],pairs[36,12],1-1[36,21],pairs[36,12],sorted_index[42,15],sorted_bag[43,17]\n",
      "2018-02-05 15:38:50,191 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 0% complete\n",
      "2018-02-05 15:39:21,752 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 6% complete\n",
      "2018-02-05 15:39:30,897 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 10% complete\n",
      "2018-02-05 15:39:36,558 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 14% complete\n",
      "2018-02-05 15:39:46,343 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 20% complete\n",
      "2018-02-05 15:39:57,901 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 24% complete\n",
      "2018-02-05 15:40:10,131 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 29% complete\n",
      "2018-02-05 15:40:19,405 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 33% complete\n",
      "2018-02-05 15:40:32,339 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 38% complete\n",
      "2018-02-05 15:40:38,528 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 45% complete\n",
      "2018-02-05 15:40:42,792 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 50% complete\n",
      "2018-02-05 15:40:59,465 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 83% complete\n",
      "2018-02-05 15:41:26,448 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 87% complete\n",
      "2018-02-05 15:41:50,963 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 92% complete\n",
      "2018-02-05 15:42:14,646 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 96% complete\n",
      "2018-02-05 15:42:46,282 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2018-02-05 15:42:51,831 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 100% complete\n",
      "2018-02-05 15:42:51,998 [main] INFO  org.apache.pig.tools.pigstats.SimplePigStats - Script Statistics: \n",
      "\n",
      "HadoopVersion\tPigVersion\tUserId\tStartedAt\tFinishedAt\tFeatures\n",
      "2.6.0-cdh5.9.0\t0.12.0-cdh5.9.0\troot\t2018-02-05 15:38:41\t2018-02-05 15:42:51\tGROUP_BY,FILTER\n",
      "\n",
      "Success!\n",
      "\n",
      "Job Stats (time in seconds):\n",
      "JobId\tMaps\tReduces\tMaxMapTime\tMinMapTIme\tAvgMapTime\tMedianMapTime\tMaxReduceTime\tMinReduceTime\tAvgReduceTime\tMedianReducetime\tAlias\tFeature\tOutputs\n",
      "job_1517830644613_0003\t1\t1\t98\t98\t98\t98\t113\t113\t113\t113\t1-1,cleandata,cleandata_filtered,data,index,pairs,sorted_bag,sorted_index,word_groups,words_data,words_data_filtered\tGROUP_BY,DISTINCT\thdfs://namenode:8020/user/root/inverted_index,\n",
      "\n",
      "Input(s):\n",
      "Successfully read 204617 records (120109499 bytes) from: \"hdfs://namenode:8020/user/root/forum_node.tsv\"\n",
      "\n",
      "Output(s):\n",
      "Successfully stored 185540 records (86936662 bytes) in: \"hdfs://namenode:8020/user/root/inverted_index\"\n",
      "\n",
      "Counters:\n",
      "Total records written : 185540\n",
      "Total bytes written : 86936662\n",
      "Spillable Memory Manager spill count : 0\n",
      "Total bags proactively spilled: 0\n",
      "Total records proactively spilled: 0\n",
      "\n",
      "Job DAG:\n",
      "job_1517830644613_0003\n",
      "\n",
      "\n",
      "2018-02-05 15:42:53,245 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Success!\n"
     ]
    }
   ],
   "source": [
    "! pig -f students-inverted-index.pig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   3 root supergroup          0 2018-02-05 15:42 inverted_index/_SUCCESS\r\n",
      "-rw-r--r--   3 root supergroup   86936662 2018-02-05 15:42 inverted_index/part-r-00000\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls inverted_index/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t{(2010230)}\t1\r\n",
      "reciprocals\t{(2009620),(2009707),(2010947),(2014292),(9002395)}\t5\r\n",
      "reciprocate\t{(2001902),(6004447)}\t2\r\n",
      "reciprocity\t{(2008341),(2009563),(2009571),(2009804),(2010292)}\t5\r\n",
      "recitations\t{(2018236),(5014249)}\t2\r\n",
      "reclamation\t{(10438)}\t1\r\n",
      "recliningon\t{(6022210)}\t1\r\n",
      "recognisers\t{(5007198),(5007700),(5007756)}\t3\r\n",
      "recognising\t{(1014977),(1034000),(6015555),(8000513)}\t4\r\n",
      "recognition\t{(3982),(5301),(8066),(9102),(22789),(41152),(47260),(51450),(51559),(52330),(53802),(53881),(53962),(60657),(60916),(63420),(64321),(64470),(66804),(66874),(67092),(67121),(67242),(67488),(1000219),(1000901),(1001733),(1001920),(1002371),(1005129),(1006927),(1007272),(1008146),(1008955),(1008996),(1009010),(1009776),(1010107),(1010329),(1010351),(1012085),(1012836),(1013518),(1013848),(1014107),(1015692),(1018390),(1023592),(1025649),(1026946),(1028196),(1030214),(1030460),(1030646),(1030651),(1031191),(1031238),(1031734),(1032697),(1032720),(1033371),(1033635),(1033857),(1034081),(1034451),(1034946),(1034954),(1035052),(1035056),(1035061),(1035176),(1035289),(2001357),(2006024),(2013591),(2016523),(2016895),(3000200),(3000463),(3001489),(3001492),(3001645),(3001837),(5004737),(5011243),(5014578),(5014587),(6000004),(6021164),(6031048),(7002749),(7003833),(7007679),(8001664),(8002598),(8004135),(8005177),(10000159),(10000366),(10000718),(10001470),(10002665),(10003051),(10003285),(10003312),(10005751),(10006251),(10006449),(10006944),(10007030),(10007571),(10007850),(10010554),(10010641),(10011519),(12003284),(12003362)}\t117\r\n",
      "tail: write error: Broken pipe\r\n",
      "tail: write error\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -cat inverted_index/part-r-00000 | tail -40000 | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
